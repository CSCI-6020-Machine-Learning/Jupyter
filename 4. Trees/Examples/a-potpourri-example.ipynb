{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary and Illustration of Machine Learning Algorithm\n",
    "\n",
    "This document illustrates succintly various machine learning algorithms. The Table of Contents below indicates the scope of this document.\n",
    "\n",
    "This is a living document. Therefore, it will undergo changes to improve its readability and coorect any typos/errors discovered.\n",
    "\n",
    "To avoid redundancy and to keep the size of notebook cells small, the cells are self-contained. For example, data is read in one cell and is used in many different cells that follow. If we do not execute the cell where the data is loaded into variables, and execute the cells that follow will result in errors. It is best to execute cells in a sequence unless you can figure out the data dependenciies and execute the relevant cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Pima Indians Diabetes Dataset](#pima_indians_diabetes_dataset)\n",
    "2. [Data Summaries](#data_summaries)\n",
    "3. [Data Visualization with Pandas](#data_visualization_with_pandas)\n",
    "4. [Data Cleaning](#data_cleaning)\n",
    "5. [Resampling Methods](#resampling_methods)\n",
    "6. [Classification Algorithms](#classification_algorithms)\n",
    "   1. [Classification/Decision Trees](#classification_decision_trees)\n",
    "   2. [Naive Bayes](#naive_bayes)\n",
    "   3. [K-Nearest Neighbors](#k_nearest_neighbors)\n",
    "   4. [Linear Discriminant Analysis](#linear_discriminant_analysis)\n",
    "   5. [Quadratic Discriminant Analysis](#quadratic_discriminant_analysis)\n",
    "   6. [Logistic Regression](#logistic_regression)\n",
    "   7. [Perceptron](#perceptron)\n",
    "   8. [Support Vector Machines](#support_vector_machines)\n",
    "   9. [Comparing Classification Algorithms ](#comparing_classification_algorithms)\n",
    "   10. [Metrics for Evaluating Classification Algorithms](#metrics-for-evaluating-classification-algorithms)\n",
    "7. [Regression Algorithms](#regression_algorithms)\n",
    "   1. [Regression Trees](#regression_trees)\n",
    "   1. [K-Nearest Neighbors](#k_nearest_neighbors)\n",
    "   1. [LassoLars](#lassoLars)\n",
    "   1. [Linear](#linear)\n",
    "   1. [Ridge](#ridge)\n",
    "   1. [Lasso](#lasso)\n",
    "   1. [Support Vector Machines](#support_vector_machines)\n",
    "   1. [Least Angle](#least_angle)\n",
    "   1. [ElasticNet](#elasticNet)\n",
    "   1. [Metrics for Evaluating Regression Algorithms](#metrics_for_evaluating_regression_algorithms)\n",
    "8. [Creating Machine Learning Algorithm Pipelines](#algorithm_pipelines)\n",
    "9. [Improving Model Performance](#improving_model_performance)\n",
    "10. [Ensemble Models for Regression](#ensemble_models_for_regression)\n",
    "11. [Ensemble Models for Classification](#ensemble_models_for_classification)\n",
    "12. [Saving and Retrieving Models](#saving_and_retrieving_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pima Indians Diabetes Dataset <a name=\"pima_indians_diabetes_dataset\"></a>\n",
    "\n",
    "We use this dataset [diabetes.csv](diabetes.csv) to illustarte some machine learning algorithms.\n",
    "\n",
    "**Predictor variables**:\n",
    "\n",
    "    - Pregnancies: Number of times pregnant\n",
    "    \n",
    "    - Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "    \n",
    "    - BloodPressure: Diastolic blood pressure (mm Hg)\n",
    "    \n",
    "    - SkinThickness: Triceps skin fold thickness (mm)\n",
    "    \n",
    "    - Insulin: 2-Hour serum insulin (mu U/ml)\n",
    "    \n",
    "    - BMI: Body mass index (weight in kg/(height in m)^2)\n",
    "    \n",
    "    - DiabetesPedigreeFunction: Diabetes pedigree function\n",
    "    \n",
    "    - Age: Age in years\n",
    "\n",
    "**Target variable**: \n",
    "\n",
    "    - Outcome: Class variable (0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas for data manipulation\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import scipy\n",
    "import seaborn\n",
    "from pandas.tools.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Summaries <a name=\"data_summaries\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read Pima Indians diabetes data\n",
    "myNames = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
    "\n",
    "data = pd.read_csv('diabetes.csv', skiprows=1, names=myNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data information\n",
    "info = data.info()\n",
    "\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore first 20 data instances\n",
    "print(data.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data types of predictor and traget variables\n",
    "types = data.dtypes\n",
    "\n",
    "# print data types\n",
    "print(types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data dimensions\n",
    "shape = data.shape\n",
    "\n",
    "print(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class counts\n",
    "print(data.groupby('Outcome').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.width', 100)\n",
    "pd.set_option('precision', 3)\n",
    "\n",
    "# statistical summary of variables\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute pairwise Spearman correlations\n",
    "cor = data.corr(method='spearman')\n",
    "\n",
    "# print correlation coefficents\n",
    "print(cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute skew for each attribute\n",
    "print(data.skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization with Pandas <a name=\"data_visualization_with_pandas\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# box and whisker plots\n",
    "data.plot(kind='box', subplots=True, layout=(3,3), sharex=False, sharey=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix plot\n",
    "correlations = data.corr()\n",
    "\n",
    "# plot correlation matrix\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(correlations, vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix plot\n",
    "correlations = data.corr()\n",
    "\n",
    "# plot correlation matrix\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(correlations, vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "ticks = numpy.arange(0,9,1)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(myNames)\n",
    "ax.set_yticklabels(myNames)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation matrix plot\n",
    "\n",
    "matplotlib.style.use('ggplot')\n",
    "correlations = data.corr()\n",
    "seaborn.heatmap(correlations)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate density plots\n",
    "data.plot(kind='density', subplots=True, layout=(3,3), sharex=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate histograms\n",
    "data.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplot matrix\n",
    "pd.plotting.scatter_matrix(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning <a name=\"data_cleaning\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data type conversion\n",
    "\n",
    "# before conversion\n",
    "print(data.dtypes)\n",
    "\n",
    "# convert datafrmae to float\n",
    "data = data.astype(float)\n",
    "\n",
    "# print two blank lines\n",
    "print('\\n' * 2)\n",
    "\n",
    "# after conversion\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# breast-cancer-wisconsin.csv - a breast cancer dataset\n",
    "myNames = ['Code', 'Clump-Thickness', 'Cell-Size', 'Cell-Shape', 'Adhesion', 'Single-Cell-Size', 'Bare-Nuclei', 'Chromatin', 'Nucleoli', 'Mitoses', 'Class']\n",
    "data2 = pd.read_csv('breast-cancer-wisconsin.csv', names=myNames)\n",
    "\n",
    "# dataset size\n",
    "print(data2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace missing data (coded as ?) with NaN\n",
    "data2[['Bare-Nuclei']] = data2[['Bare-Nuclei']].replace('?', numpy.NaN)\n",
    "\n",
    "# drop rows that have missing data\n",
    "data2.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "# dataset size\n",
    "print(data2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete a column\n",
    "\n",
    "# size of the dataset before deltion of a column\n",
    "print(data2.shape)\n",
    "\n",
    "# delete coumn named Outcome\n",
    "data2.drop('Adhesion', axis=1, inplace=True)\n",
    "\n",
    "# size of the dataset after deltion of a column\n",
    "print(data2.shape)\n",
    "\n",
    "print(data2.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection with extra trees classifier\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "print(data.head(4))\n",
    "print('\\n')\n",
    "\n",
    "# convert dataframe to an array\n",
    "array = data.values\n",
    "\n",
    "# predictor variables - first eight columns (0 through 7)\n",
    "X = array[:,0:8]\n",
    "\n",
    "print(X)\n",
    "print('\\n')\n",
    "\n",
    "# target variable\n",
    "Y = array[:,8]\n",
    "\n",
    "print(Y)\n",
    "print('\\n')\n",
    "\n",
    "# feature extraction using extra trees classifier\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X, Y)\n",
    "\n",
    "# important features\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify features with low variance\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# feature selection\n",
    "threshold = 0.8 * (1 - 0.8)\n",
    "\n",
    "test = VarianceThreshold(threshold)\n",
    "fit = test.fit(X)\n",
    "\n",
    "print(fit.variances_)\n",
    "print('\\n')\n",
    "\n",
    "features = fit.transform(X)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection with Principal Component Analysis (PCA)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# feature extraction\n",
    "pca = PCA(n_components=3)\n",
    "pcaModel = pca.fit(X)\n",
    "\n",
    "# summarize PCA components\n",
    "# print(\"Explained variance: %s\") % pcaModel.explained_variance_ratio_\n",
    "print('\\n')\n",
    "\n",
    "print(pcaModel.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recursive feature extraction\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# recursive feature extraction\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, 3)\n",
    "\n",
    "fit = rfe.fit(X, Y)\n",
    "\n",
    "print(\"Number of features: %d\") % fit.n_features_\n",
    "print('\\n')\n",
    "\n",
    "print(\"Selected features: %s\") % fit.support_\n",
    "print('\\n')\n",
    "\n",
    "print(\"Feature ranking: %s\") % fit.ranking_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extraction with univariate statistical tests (Chi-squared for classification)\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# feature extraction\n",
    "test = SelectKBest(score_func=chi2, k=4)\n",
    "fit = test.fit(X, Y)\n",
    "\n",
    "# summarize scores\n",
    "numpy.set_printoptions(precision=3)\n",
    "print(fit.scores_)\n",
    "print('\\n')\n",
    "\n",
    "features = fit.transform(X)\n",
    "\n",
    "# summarize selected features\n",
    "print(features[0:5,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarization\n",
    "\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "binarizer = Binarizer(threshold=0.0).fit(X)\n",
    "\n",
    "binaryX = binarizer.transform(X)\n",
    "\n",
    "# summarize transformed data\n",
    "numpy.set_printoptions(precision=3)\n",
    "print(binaryX[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box-Cox transform\n",
    "\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "X_boxcox = boxcox(1+X[:,2])[0]\n",
    "print(X_boxcox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sonar Dataset\n",
    "\n",
    "This dataset has 208 instances and 60 variables.Based on the sonar signal characteristics, the goal is to predict whether the object is a rock (M) or mine (M)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a string class label to an integer\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# sonar dataset\n",
    "data3 = pd.read_csv('sonar.csv', header=None)\n",
    "\n",
    "# data3 dimensions\n",
    "print(data3.shape)\n",
    "print('\\n')\n",
    "\n",
    "# explore first 5  data3 instances\n",
    "print(data3.head(5))\n",
    "print('\\n')\n",
    "\n",
    "# explore last 5  data3 instances\n",
    "print(data3.tail(5))\n",
    "print('\\n')\n",
    "\n",
    "# dataframe to array\n",
    "array = data3.values\n",
    "\n",
    "# all rows, but first 60 columns\n",
    "y = array[:, 60]\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "print(encoder.classes_)\n",
    "print('\\n')\n",
    "\n",
    "encoded_y = encoder.transform(y)\n",
    "print(encoded_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values with mean attribute values\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "X[X == 0] = numpy.nan\n",
    "\n",
    "print(X)\n",
    "print('\\n')\n",
    "\n",
    "# replace missing values (NaN) with mean values of the variable\n",
    "imputer = Imputer(missing_values='NaN', strategy='mean')\n",
    "\n",
    "# perform imputation\n",
    "imputedX = imputer.fit_transform(X)\n",
    "print(imputedX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data (length of 1)\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# restore original data - read Pima Indians diabetes data\n",
    "myNames = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
    "data = pd.read_csv('diabetes.csv', skiprows=1, names=myNames)\n",
    "\n",
    "# datafrmae to array\n",
    "array = data.values\n",
    "\n",
    "# separate array into input and output components\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "scaler = Normalizer().fit(X)\n",
    "normalizedX = scaler.transform(X)\n",
    "\n",
    "# summarize transformed data\n",
    "numpy.set_printoptions(precision=3)\n",
    "print(normalizedX[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale data (between 0 and 1) - you may choose other values for 0 and 1\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaledX = scaler.fit_transform(X)\n",
    "\n",
    "# summarize transformed data\n",
    "numpy.set_printoptions(precision=3)\n",
    "print(rescaledX[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize data (0 mean, 1 stdev)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X)\n",
    "rescaledX = scaler.transform(X)\n",
    "\n",
    "# summarize transformed data\n",
    "numpy.set_printoptions(precision=3)\n",
    "print(rescaledX[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling Methods <a name=\"resampling_methods\"></a>\n",
    "\n",
    "- Train/Test Split\n",
    "\n",
    "- Shuffle Split\n",
    "\n",
    "- Cross Validation (CV)\n",
    "\n",
    "- Leave One Out Cross Validation (LOOCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a model by splitting data into training and test sets\n",
    "# Train/Test Split\n",
    "\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# divide the dataset into training (67%) and test (33%) sets\n",
    "X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(X, Y, test_size=0.33, random_state=7)\n",
    "model = LogisticRegression()\n",
    "\n",
    "# build the model using the training data set\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# evaluate the model using test dataset\n",
    "result = model.score(X_test, Y_test)\n",
    "print(\"Accuracy: %.3f\" % (result*100.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle split\n",
    "\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "instances = len(X)\n",
    "# 67% of data used for building the model and 33% for testing\n",
    "kfold = cross_validation.ShuffleSplit(n=instances, n_iter=10, test_size=0.33, random_state=7)\n",
    "model = LogisticRegression()\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Accuracy: %.3f%% (%.3f)\" % (results.mean()*100.0, results.std()*100.0))\n",
    "# print(\"Accuracy: %.3f (.3f)\" % (results.mean()*100.0, results.std()*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation (CV)\n",
    "\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "instances = len(X)\n",
    "\n",
    "kfold = cross_validation.KFold(n=instances, n_folds=10, random_state=7)\n",
    "model = LogisticRegression()\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold)\n",
    "\n",
    "print(\"Accuracy: %.3f%% (%.3f)\" % (results.mean()*100.0, results.std()*100.0))\n",
    "# print(\"Accuracy: %.3f\" % (results*100.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave One Out Cross Validation (LOOCV)\n",
    "\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "instances = len(X)\n",
    "\n",
    "loocv = cross_validation.LeaveOneOut(n=instances)\n",
    "model = LogisticRegression()\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=loocv)\n",
    "\n",
    "print(\"Accuracy: %.3f%% (%.3f)\" % (results.mean()*100.0, results.std()*100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Algorithms <a name=\"classification_algorithms\"></a>\n",
    "\n",
    "- Classification/Decision Trees\n",
    "\n",
    "- Naive Bayes\n",
    "\n",
    "- K-Nearest Neighbors\n",
    "\n",
    "- Linear Discriminant Analysis\n",
    "\n",
    "- Quadratic Discriminant Analysis\n",
    "\n",
    "- Logistic Regression\n",
    "\n",
    "- Perceptron\n",
    "\n",
    "- Support Vector Machines\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "We will use the same dataset (ima Indians diabetes) for all clasifcation algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# read Pima Indians diabetes data\n",
    "# myNames = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']\n",
    "myNames = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "data = pd.read_csv('diabetes.csv', skiprows=1, names=myNames)\n",
    "\n",
    "# convert dataframe to an array\n",
    "array = data.values\n",
    "\n",
    "# predictor variables - first eight columns (0 through 7)\n",
    "X = array[:,0:8]\n",
    "\n",
    "# target variable\n",
    "Y = array[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification/Decision Trees <a name=\"classification_decision_trees\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = len(X)\n",
    "\n",
    "kfold = cross_validation.KFold(n=instances, n_folds=10, random_state=7)\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold)\n",
    "\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes <a name=\"naive_bayes\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "instances = len(X)\n",
    "kfold = cross_validation.KFold(n=instances, n_folds=10, random_state=7)\n",
    "\n",
    "model = GaussianNB()\n",
    "\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors <a name=\"k_nearest_neighbors\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "instances = len(X)\n",
    "\n",
    "kfold = cross_validation.KFold(n=instances, n_folds=10, random_state=7)\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold)\n",
    "\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis <a name=\"linear_discriminant_analysis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "instances = len(X)\n",
    "kfold = cross_validation.KFold(n=instances, n_folds=10, random_state=7)\n",
    "model = LinearDiscriminantAnalysis()\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Discriminant Analysis <a name=\"quadratic_discriminant_analysis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "instances = len(X)\n",
    "kfold = cross_validation.KFold(n=instances, n_folds=10, random_state=7)\n",
    "\n",
    "model = QuadraticDiscriminantAnalysis()\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold)\n",
    "\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression <a name=\"logistic_regression\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "instances = len(X)\n",
    "kfold = cross_validation.KFold(n=instances, n_folds=10, random_state=7)\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron <a name=\"perceptron\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "instances = len(X)\n",
    "kfold = cross_validation.KFold(n=instances, n_folds=10, random_state=7)\n",
    "\n",
    "model = Perceptron()\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines <a name=\"support_vector_machines\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "instances = len(X)\n",
    "kfold = cross_validation.KFold(n=instances, n_folds=10, random_state=7)\n",
    "\n",
    "model = SVC()\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Classification Algorithms <a name=\"comparing_classification_algorithms\"></a>\n",
    "\n",
    "We will compare the performance of the following algorithms using Pima Indians Diabetes dataset:\n",
    "\n",
    "- Logistic Regression\n",
    "\n",
    "- K-Nearest Neighbors\n",
    "\n",
    "- Linear Discriminant Analysis\n",
    "\n",
    "- Decision Tree Classifier\n",
    "\n",
    "- Naive Bayes\n",
    "\n",
    "- Support Vector Machines\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of classification models\n",
    "models = []\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('QDA', QuadraticDiscriminantAnalysis()))\n",
    "models.append(('LR', LogisticRegression()))\n",
    "# models.append(('PR', Perceptron()))\n",
    "models.append(('SVM', SVC()))\n",
    "\n",
    "# we will evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "instances = len(X)\n",
    "\n",
    "# for each model, compute mean accuracy and standard deviation\n",
    "for name, model in models:\n",
    "   kfold = cross_validation.KFold(n=instances, n_folds=10, random_state=7)\n",
    "   cv_results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring='accuracy')\n",
    "   results.append(cv_results)\n",
    "   names.append(name)\n",
    "   msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "   print(msg)\n",
    "    \n",
    "# visualize accuracy results using boxplots\n",
    "figure = plt.figure()\n",
    "figure.suptitle('Comparison of Accuracy of Classification Algorithms')\n",
    "ax = figure.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for Evaluating Classification Algorithms <a name=\"metrics-for-evaluating-classification-algorithms\"></a>\n",
    "\n",
    "- Accuracy\n",
    "\n",
    "- Confusion Matrix\n",
    "\n",
    "- Area Under the Curve (AUC)\n",
    "\n",
    "- F1\n",
    "\n",
    "- LogLoss\n",
    "\n",
    "- Classification Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "instances = len(X)\n",
    "\n",
    "kfold = cross_validation.KFold(n=instances, n_folds=10, random_state=7)\n",
    "model = LogisticRegression()\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring='accuracy')\n",
    "print(\"Accuracy: %.3f %.3f\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# divide data into training (67%) and test sets (33%)\n",
    "X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(X, Y, test_size=0.33, random_state=7)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "predicted = model.predict(X_test)\n",
    "matrix = confusion_matrix(Y_test, predicted)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area Under the Curve (AUC)\n",
    "\n",
    "instances = len(X)\n",
    "kfold = cross_validation.KFold(n=instances, n_folds=10, random_state=7)\n",
    "model = LogisticRegression()\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring='roc_auc')\n",
    "print(\"AUC: %.3f %.3f\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1\n",
    "\n",
    "instances = len(X)\n",
    "kfold = cross_validation.KFold(n=instances, n_folds=10, random_state=7)\n",
    "model = LogisticRegression()\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring='f1')\n",
    "print(\"F1: %.3f %.3f\" % (results.mean(), results.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogLoss\n",
    "\n",
    "instances = len(X)\n",
    "kfold = cross_validation.KFold(n=instances, n_folds=10, random_state=7)\n",
    "model = LogisticRegression()\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring='neg_log_loss')\n",
    "print(\"LogLoss: %.3f %.3f\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# divide data into training (67%) and test sets (33%)\n",
    "X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(X, Y, test_size=0.33, random_state=7)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "predicted = model.predict(X_test)\n",
    "\n",
    "report = classification_report(Y_test, predicted)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Algorithms <a name=\"regression_algorithms\"></a>\n",
    "\n",
    "- Regression Trees\n",
    "\n",
    "- K-Nearest Neighbors\n",
    "\n",
    "- LassoLars\n",
    "\n",
    "- Linear\n",
    "\n",
    "- Ridge\n",
    "\n",
    "- Lasso\n",
    "\n",
    "- Support Vector Machines\n",
    "\n",
    "- Least Angle\n",
    "\n",
    "- ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boston Housing Dataset <a name=\"boston_housing_dataset\"></a>\n",
    "\n",
    "We will use [Boston Housing dataset](housing.dat) for illustrating some machine learning algorithms. It is not a CSV file. Columns are seperated by whitespace.\n",
    "\n",
    "Predictor variables are:\n",
    "\n",
    "- **crim**: per capita crime rate by town.\n",
    "\n",
    "- **zn**: proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "\n",
    "- **indus:**:  proportion of non-retail business acres per town.\n",
    "\n",
    "- **chas**: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n",
    "\n",
    "- **nox**: nitrogen oxides concentration (parts per 10 million).\n",
    "\n",
    "- **rm**: average number of rooms per dwelling.\n",
    "\n",
    "- **age**: proportion of owner-occupied units built prior to 1940.\n",
    "\n",
    "- **dis**: weighted mean of distances to five Boston employment centres.\n",
    "\n",
    "- **rad**: index of accessibility to radial highways.\n",
    "\n",
    "- **tax**: full-value property-tax rate per $10,000.\n",
    "\n",
    "- **ptratio**: pupil-teacher ratio by town.\n",
    "\n",
    "- **black**: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.\n",
    "\n",
    "- **lstat**: lower status of the population (percent).\n",
    "\n",
    "\n",
    "\n",
    "Target variable is: \n",
    "\n",
    "- **medv**: median value of owner-occupied homes in $1000s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "\n",
    "print(boston.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boston.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boston.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data types of predictor variables\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data into a dataframe\n",
    "bData = pd.DataFrame(boston.data)\n",
    "\n",
    "# print first few lines\n",
    "print(bData.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bData = bData.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data types of variables\n",
    "print(bData.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# associate names to columns\n",
    "bData.columns = boston.feature_names\n",
    "print(bData.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target attribute, price, is available in target attribute\n",
    "print(boston.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add target attribute to the dataframe as price column\n",
    "bData['PRICE'] = boston.target\n",
    "print(bData.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary statistics\n",
    "print(bData.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data types of predictor and target variables\n",
    "print(bData.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert dataframe to an array\n",
    "array = data.values\n",
    "\n",
    "# predictor variables - first eight columns (0 through 7)\n",
    "# X = array[:,0:13]\n",
    "X = bData.drop('PRICE', axis=1)\n",
    "\n",
    "# target variable\n",
    "# Y = array[:,13]\n",
    "Y = bData['PRICE']\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Trees <a name=\"regression_trees\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "instances = len(X)\n",
    "\n",
    "kfold = cross_validation.KFold(n=instances, n_folds=10, random_state=7)\n",
    "model = DecisionTreeRegressor()\n",
    "\n",
    "# results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring='mean_squared_error')\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors <a name=\"k_nearest_neighbors\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "instances = len(X)\n",
    "\n",
    "kfold = cross_validation.KFold(n=instances, n_folds=10, random_state=7)\n",
    "model = KNeighborsRegressor()\n",
    "\n",
    "# results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring='mean_squared_error')\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear <a name=\"linear\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "instances = len(X)\n",
    "\n",
    "kfold = cross_validation.KFold(n=instances, n_folds=10, random_state=7)\n",
    "model = LinearRegression()\n",
    "\n",
    "# results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring='mean_squared_error')\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LassoLars <a name=\"lassolars\"></a>\n",
    "\n",
    "Lars with LASSO modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoLars\n",
    "\n",
    "instances = len(X)\n",
    "\n",
    "kfold = cross_validation.KFold(n=instances, n_folds=10, random_state=7)\n",
    "model = LassoLars()\n",
    "\n",
    "# results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring='mean_squared_error')\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge <a name=\"ridge\"></a>\n",
    "\n",
    "Suitable for analyzing multiple regression data that suffer from multicollinearity. One predictor variable can be linearly predicted from the others with a great degree of accuracy.\n",
    "\n",
    "When multicollinearity occurs, least squares estimates are unbiased, but their variances are large. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "instances = len(X)\n",
    "\n",
    "kfold = cross_validation.KFold(n=instances, n_folds=10, random_state=7)\n",
    "model = Ridge()\n",
    "\n",
    "# results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring='mean_squared_error')\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso <a name=\"lasso\"></a>\n",
    "\n",
    "Performs both variable selection and regularization to improve the prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "instances = len(X)\n",
    "\n",
    "kfold = cross_validation.KFold(n=instances, n_folds=10, random_state=7)\n",
    "model = Lasso()\n",
    "\n",
    "# results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring='mean_squared_error')\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet <a name=\"elasticnet\"></a>\n",
    "ElasticNet is a regularization regression that combines the properties of both Ridge Regression and LASSO regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "instances = len(X)\n",
    "\n",
    "kfold = cross_validation.KFold(n=instances, n_folds=10, random_state=7)\n",
    "model = ElasticNet()\n",
    "\n",
    "# results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring='mean_squared_error')\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines <a name=\"support_vector_machines\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "instances = len(X)\n",
    "\n",
    "kfold = cross_validation.KFold(n=instances, n_folds=10, random_state=7)\n",
    "model = SVR()\n",
    "\n",
    "# results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring='mean_squared_error')\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Angle <a name=\"least_angle\"></a>\n",
    "\n",
    "Suitable for developing linear regression models for high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lars\n",
    "\n",
    "instances = len(X)\n",
    "\n",
    "kfold = cross_validation.KFold(n=instances, n_folds=10, random_state=7)\n",
    "model = Lars()\n",
    "\n",
    "# results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring='mean_squared_error')\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for Evaluating Regression Algorithms <a name=\"metrics_for_evaluating_regression_algorithms\"></a>\n",
    "\n",
    "- $R^2$\n",
    "\n",
    "- Negative Mean Absolute Error (MAE)\n",
    "\n",
    "- Mean Squared Error (MSE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R^2\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "instances = len(X)\n",
    "kfold = cross_validation.KFold(n=instances, n_folds=10, random_state=7)\n",
    "model = LinearRegression()\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring='r2')\n",
    "print(\"R^2: %.3f %.3f\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Absolute Error (MAE)\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "instances = len(X)\n",
    "kfold = cross_validation.KFold(n=instances, n_folds=10, random_state=7)\n",
    "model = LinearRegression()\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring='neg_mean_absolute_error')\n",
    "print(\"MAE: %.3f %.3f\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Squared Error (MSE)\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "instances = len(X)\n",
    "kfold = cross_validation.KFold(n=instances, n_folds=10, random_state=7)\n",
    "model = LinearRegression()\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "print(\"MSE: %.3f %.3f\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Machine Learning Algorithm Pipelines <a name=\"algorithm_pipelines\"></a>\n",
    "\n",
    "**Feature union**: select best features first (for example, using PCA) and then build models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# create feature union\n",
    "features = []\n",
    "features.append(('pca', PCA(n_components=3)))\n",
    "features.append(('select_best', SelectKBest(k=6)))\n",
    "feature_union = FeatureUnion(features)\n",
    "\n",
    "# create a pipeline\n",
    "estimators = []\n",
    "estimators.append(('feature_union', feature_union))\n",
    "estimators.append(('logistic', LogisticRegression()))\n",
    "\n",
    "model = Pipeline(estimators)\n",
    "\n",
    "# evaluate pipeline\n",
    "instances = len(X)\n",
    "kfold = KFold(n=instances, n_folds=10, random_state=7)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pipeline to standardize the data first and then creates a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# create pipeline\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('lda', LinearDiscriminantAnalysis()))\n",
    "model = Pipeline(estimators)\n",
    "\n",
    "# evaluate pipeline\n",
    "instances = len(X)\n",
    "kfold = KFold(n=instances, n_folds=10, random_state=7)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Model Performance <a name=\"improving_model_performance\"></a>\n",
    "\n",
    "- Grid Search\n",
    "\n",
    "- Randomized Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search\n",
    "\n",
    "import pandas\n",
    "import numpy\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "alphas = numpy.array([1,0.1,0.01,0.001,0.0001,0])\n",
    "\n",
    "grid = GridSearchCV(estimator=Ridge(), param_grid=dict(alpha=alphas))\n",
    "grid.fit(X, Y)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomized Search\n",
    "\n",
    "import pandas\n",
    "import numpy\n",
    "from scipy.stats import uniform\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_grid = {'alpha': uniform()}\n",
    "rsearch = RandomizedSearchCV(estimator=Ridge(), param_distributions=param_grid, n_iter=100, random_state=7)\n",
    "rsearch.fit(X, Y)\n",
    "print(rsearch.best_score_)\n",
    "print(rsearch.best_estimator_.alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Models for Regression <a name=\"ensemble_models_for_regression\"></a>\n",
    "\n",
    "- Gradient Boosting\n",
    "    \n",
    "- Adaboost\n",
    "    \n",
    "- Random Forest\n",
    "    \n",
    "- Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting regression\n",
    "\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "instances = len(X)\n",
    "kfold = cross_validation.KFold(n=instances, n_folds=10, random_state=7)\n",
    "model = GradientBoostingRegressor()\n",
    "\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring='mean_squared_error')\n",
    "print(results.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaboost regression\n",
    "\n",
    "instances = len(X)\n",
    "kfold = cross_validation.KFold(n=instances, n_folds=10, random_state=7)\n",
    "model = AdaBoostRegressor()\n",
    "\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring='mean_squared_error')\n",
    "print(results.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest regression\n",
    "\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "instances = len(X)\n",
    "kfold = cross_validation.KFold(n=instances, n_folds=10, random_state=7)\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring='mean_squared_error')\n",
    "print(results.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra Trees regression\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "instances = len(X)\n",
    "kfold = cross_validation.KFold(n=instances, n_folds=10, random_state=7)\n",
    "model = ExtraTreesRegressor()\n",
    "\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold, scoring='mean_squared_error')\n",
    "print(results.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Models for Classification <a name=\"ensemble_models_for_classification\"></a>\n",
    "\n",
    "- Bagging\n",
    "\n",
    "- Gradient Boosting\n",
    "    \n",
    "- Adaboost\n",
    "    \n",
    "- Random Forest\n",
    "    \n",
    "- Extra Trees\n",
    "\n",
    "- Voting Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging classifier\n",
    "\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "instances = len(X)\n",
    "\n",
    "kfold = cross_validation.KFold(n=instances, n_folds=10, random_state=7)\n",
    "cart = DecisionTreeClassifier()\n",
    "num_trees = 100\n",
    "model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=7)\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting classifier\n",
    "\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "instances = len(X)\n",
    "num_trees = 100\n",
    "kfold = cross_validation.KFold(n=instances, n_folds=num_folds, random_state=7)\n",
    "model = GradientBoostingClassifier(n_estimators=num_trees, random_state=7)\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adaboost classifier\n",
    "\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "instances = len(X)\n",
    "num_trees = 30\n",
    "kfold = cross_validation.KFold(n=instances, n_folds=num_folds, random_state=7)\n",
    "model = AdaBoostClassifier(n_estimators=num_trees, random_state=7)\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest classifier\n",
    "\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "instances = len(X)\n",
    "num_trees = 100\n",
    "kfold = cross_validation.KFold(n=instances, n_folds=num_folds, random_state=seed)\n",
    "model = RandomForestClassifier(n_estimators=num_trees, max_features=3)\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra Trees classifier\n",
    "\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "instances = len(X)\n",
    "num_trees = 100\n",
    "kfold = cross_validation.KFold(n=instances, n_folds=10, random_state=7)\n",
    "model = ExtraTreesClassifier(n_estimators=num_trees, max_features=7)\n",
    "results = cross_validation.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting Ensemble classifier\n",
    "\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "instances = len(X)\n",
    "kfold = cross_validation.KFold(n=instances, n_folds=10, random_state=7)\n",
    "\n",
    "# create the submodels\n",
    "estimators = []\n",
    "\n",
    "# losgistic regression classifier\n",
    "model1 = LogisticRegression()\n",
    "estimators.append(('logistic', model1))\n",
    "\n",
    "# Decision tree classifier\n",
    "model2 = DecisionTreeClassifier()\n",
    "estimators.append(('cart', model2))\n",
    "\n",
    "# support vector machine classifier\n",
    "model3 = SVC()\n",
    "estimators.append(('svm', model3))\n",
    "\n",
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators)\n",
    "\n",
    "# evalauate the performance of the ensemble\n",
    "results = cross_validation.cross_val_score(ensemble, X, Y, cv=kfold)\n",
    "\n",
    "print(results.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Retrieving Models <a name=\"saving_and_retrieving_models\"></a>\n",
    "\n",
    "- joblib\n",
    "\n",
    "- Pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and retrieve model using joblib\n",
    "\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(X, Y, test_size=0.33, random_state=7)\n",
    "\n",
    "# build model using training data\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# evaluate model performance using the test data\n",
    "result = model.score(X_test, Y_test)\n",
    "print(result)\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'final_lr__model.sav'\n",
    "joblib.dump(model, filename)\n",
    "\n",
    "\n",
    "# sometime in future, retrieve the model\n",
    "loaded_model = joblib.load(filename)\n",
    "result = loaded_model.score(X_test, Y_test)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and retrieve model using Pickle\n",
    "\n",
    "import pandas\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(X, Y, test_size=0.33, random_state=7)\n",
    "\n",
    "# build model using training data\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# evaluate model performance using the test data\n",
    "result = model.score(X_test, Y_test)\n",
    "print(result)\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'final_lr__model2.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))\n",
    "\n",
    "\n",
    "# sometime in future, retrieve the model\n",
    "loaded_model_2 = joblib.load(filename)\n",
    "result = loaded_model_2.score(X_test, Y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
