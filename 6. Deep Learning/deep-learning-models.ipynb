{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning with Python\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Software Infrastructure](#software_infrastructure)\n",
    "\n",
    "1. [Reference Books for Theoretical Background](#reference_books)\n",
    "\n",
    "1. [Installing Theano](#installing_theano)\n",
    "\n",
    "1. [Installing TensorFlow](#installing_theano)\n",
    "\n",
    "1. [Installing Keras](#installing_tensorflow)\n",
    "\n",
    "1. [Perceptron](#perceptron)\n",
    "\n",
    "1. [Multilayer Feedforward Networks](#multilayer_feedforward_network)\n",
    "\n",
    "1. [Stochastic Gradient Descent](#stochastic_gradient_descent)\n",
    "\n",
    "1. [Building a Simple Neural Network Model with Keras](#building_a_simple_network_with_keras)\n",
    "\n",
    "1. [Multi Layer Perceptron (MLP) with Automatic Validation Set](#auto_validation)\n",
    "\n",
    "1. [MLP with Manual Validation Set](#manual_validation)\n",
    "\n",
    "1. [MLP with Manual k-Fold Validation Set](#manual_kfold_validation)\n",
    "\n",
    "1. [Loss Functions and Optimization Algorithms](#loss_optimiztion)\n",
    "\n",
    "1. [Grid Search Deep Learning Model Parameters](#grid_search)\n",
    "\n",
    "1. [Classification Problems](#classification_problems)\n",
    "\n",
    "1. [Improving Model Performance with Data Preparation](#improving_model_performance)\n",
    "\n",
    "1. [Regression Problems](#regression_problems)\n",
    "\n",
    "1. [Saving ML Models in HDF5, JSON, and YAML Formats with Serialization](#saving_models)\n",
    "\n",
    "1. [Checkpointing Models During Training](#check_pointing)\n",
    "\n",
    "1. [Improving Models](#improving_models)\n",
    "\n",
    "    1. [Reduce Overfitting With Dropout Regularization](#dropout_regularization)\n",
    "\n",
    "    1. [Improving Performance With Learning Rate Schedules](#learning_rate_schedules)\n",
    "\n",
    "1. [Convolutional Neural Networks (CNNs)](#cnns)\n",
    "\n",
    "1. [Long Short-Term Memory (LSTMs) Units/RNN](#lstms)\n",
    "\n",
    "1. [Combining LSTMs with CNNs](#lstm_plus_cnn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Infrastructure <a name=\"software_infrastructure\"></a>\n",
    "\n",
    "We will use the following libraries and tools:\n",
    "\n",
    "- Python 3\n",
    "- **Matplotlib**, a Python library for plotting.\n",
    "- **Pandas**, a Python library which provides a dataframe data structure, and operations for manipulating numerical tables and time series.\n",
    "- **NumPy** Python library which provides support for large, multi-dimensional arrays and matrices, as well as a large collection of high-level mathematical functions to operate on these arrays. \n",
    "- Python **SciPy** library for scientific computing -- modules for optimization, linear algebra, integration, interpolation, special functions, FFT, signal and image processing, ODE solvers and other tasks common in science and engineering.\n",
    "- Python **scikit-learn**, a machine learning library\n",
    "- **Theano**, a numerical platform for developing deep learning models\n",
    "- **TensorFlow**, another numerical platform for developing deep learning models\n",
    "- **Keras**, a concise Python API for **Theano** and **TensorFlow**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference Books for Theoretical Background <a name=\"reference_books\"></a>\n",
    "\n",
    "- Ian Goodfellow, Yoshua Bengio  and, Aaron Courville. **Deep Learning**, The MIT Press, ISBN: 978-0262035613, 2016. Click [here](https://www.amazon.com/Deep-Learning-Adaptive-Computation-Machine/dp/0262035618/ref=sr_1_3?ie=UTF8&qid=1542676662&sr=8-3&keywords=bengio+deep+learning+book) for details.\n",
    "\n",
    "- Charu C. Aggarwal. **Neural Networks and Deep Learning: A Textbook**, Springer, ISBN: 978-3319944623, 2018. Click [here](https://www.amazon.com/Neural-Networks-Deep-Learning-Textbook/dp/3319944622/ref=sr_1_2_sspa?ie=UTF8&qid=1542676662&sr=8-2-spons&keywords=bengio+deep+learning+book&psc=1) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing Theano <a name=\"installing_theano\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-05be32ceeb7a>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-05be32ceeb7a>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    sudo pip install --upgrade --no-deps theano\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# installing Theano (pip command is available through Anaconda installation)\n",
    "\n",
    "sudo pip install --upgrade --no-deps theano\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "# a simple Theano example\n",
    "\n",
    "import theano\n",
    "from theano import tensor\n",
    "\n",
    "# two symbolic floating-point scalars\n",
    "a = tensor.dscalar()\n",
    "b = tensor.dscalar()\n",
    "\n",
    "# a simple symbolic expression\n",
    "c = a + b\n",
    "\n",
    "# convert the expression into a callable object \n",
    "# which takes (a,b) and computes c\n",
    "f = theano.function([a,b], c)\n",
    "\n",
    "# bind 1.5 to 'a', 2.5 to 'b', and evaluate 'c'\n",
    "result = f(1.5, 2.5)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing TensorFlow <a name=\"installing_tensorflow\"></a>\n",
    "\n",
    "Follow the instructions [here](https://www.tensorflow.org/install/pip) for installing TensorFlow.\n",
    "\n",
    "TensorFlow installation comes with a number of Deep Learning models, which you can use and experiment with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "# a simple TensorFlow example\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# two symbolic floating-point scalars\n",
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "\n",
    "# a simple symbolic expression using the add function\n",
    "add = tf.add(a, b)\n",
    "\n",
    "# bind 1.5 to 'a', 2.5 to 'b', and evaluate 'c'\n",
    "binding = {a: 1.5, b: 2.5}\n",
    "\n",
    "session = tf.Session()\n",
    "c = session.run(add, feed_dict=binding)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras - a Python Library for Deep Learning <a name=\"installing_keras\"></a>\n",
    "\n",
    "- Keras can be run run on top of Theano or TensorFlow. Specify which one to use with Keras via a configuration file.\n",
    "\n",
    "- Seamlessly executes on GPUs and CPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do not execute the following cell if you have already installed Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial installation of Keras\n",
    "\n",
    "sudo pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upgrade current installtion of Keras\n",
    "\n",
    "sudo pip install --upgrade keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Deep Learning Models with Keras\n",
    "\n",
    "- Create a Sequential model and add configured layers.\n",
    "\n",
    "- Specify loss function and optimizer, and call the **compile()**\n",
    "function on the model.\n",
    "\n",
    "- Train the model on a sample of data by calling the **fit()** function on\n",
    "the model.\n",
    "\n",
    "- Use the model to generate predictions on new data via\n",
    "functions such as **evaluate()** or **predict()**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Perceptron <a name=\"perceptron\"></a>\n",
    "\n",
    "![](ann-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Perceptron\n",
    "\n",
    "![](ann-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Multilayer Feedforward Network <a name=\"multilayer_feedforward_network\"></a>\n",
    "\n",
    "![](ann-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Multilayer Feedforward Network\n",
    "\n",
    "![](ann-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Multilayer Neural Network for a Classification Problem\n",
    "\n",
    "![](hand-written-digit-recognition.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "\n",
    "- A simple mapping of summed weighted input to the output of the neuron.\n",
    "- Governs the threshold at which the neuron is activated and the strength of the output signal.\n",
    "- sigmoid, relu, tanh, leaky relu.\n",
    "- A regression problem may have a single output neuron and the neuron may have no\n",
    "activation function.\n",
    "- A binary classification problem may have a single output neuron and use a sigmoid activation function to output a value between 0 and 1.\n",
    "- A multiclass classification problem may have multiple neurons in the output layer, one for each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data\n",
    "\n",
    "- Must be numerical.\n",
    "\n",
    "- Categorical data such as **gender** must be converted to a real-valued representation called **one-hot** encoding.\n",
    "\n",
    "- One-hot encoding is also used for the output variable in classicacation problems\n",
    "with more than one class.\n",
    "\n",
    "- Rescaling data to the range 0 to 1 - **normalization**.\n",
    "\n",
    "- Another technique: **standardize** a feature so that its distribution has a mean of zero and a standard deviation of 1.\n",
    "\n",
    "- Words should be converted to numbers as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent <a name=\"stochastic_gradient_descent\"></a>\n",
    "\n",
    "- A training algorithm for neural networks.\n",
    "\n",
    "- Neurons get activated as an input instance goes through the network in the forward direction - **forward pass**.\n",
    "\n",
    "- Output of the network is compared to the expected output and an **error** is calculated.\n",
    "\n",
    "- Error is propagated back through the network, one layer at a time, and the weights are updated according to the amount that they contributed to the error -- **Back Propagation Algorithm**.\n",
    "\n",
    "- The above process is repeated for every instance in the training dataset.\n",
    "\n",
    "- An **epoch** is one round of updating the network for the entire training dataset.\n",
    "\n",
    "- A network may be trained for many thousands of epochs.\n",
    "\n",
    "- **Online learning** - weights in the network can be updated from the errors calculated for each training example. Fast learning, but can also result in chaotic changes to the network.\n",
    "\n",
    "- **Batch learning** - errors are saved across all of the training examples and the network is updated at the end. More stable learning.\n",
    "\n",
    "- **Learning rate** - controls the step or change made to network weights for a given error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Surface\n",
    "\n",
    "![](error-surface.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local and Global Minima\n",
    "\n",
    "![](local-and-global-minima.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent/Online Learning\n",
    "\n",
    "![](gradient-descent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Simple Neural Network Model with Keras <a name=\"building_a_simple_network_with_keras\"></a>\n",
    "\n",
    "- Visible/source layer, two hidden layers, and one output layer.\n",
    "\n",
    "- Pima Indian dataset from the UCI Machine Learning repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "768/768 [==============================] - 0s 381us/step - loss: 0.6772 - acc: 0.6510\n",
      "Epoch 2/15\n",
      "768/768 [==============================] - 0s 96us/step - loss: 0.6586 - acc: 0.6510\n",
      "Epoch 3/15\n",
      "768/768 [==============================] - 0s 99us/step - loss: 0.6472 - acc: 0.6510\n",
      "Epoch 4/15\n",
      "768/768 [==============================] - 0s 122us/step - loss: 0.6391 - acc: 0.6510\n",
      "Epoch 5/15\n",
      "768/768 [==============================] - 0s 122us/step - loss: 0.6315 - acc: 0.6510\n",
      "Epoch 6/15\n",
      "768/768 [==============================] - 0s 96us/step - loss: 0.6180 - acc: 0.6510\n",
      "Epoch 7/15\n",
      "768/768 [==============================] - 0s 93us/step - loss: 0.6195 - acc: 0.6510\n",
      "Epoch 8/15\n",
      "768/768 [==============================] - 0s 100us/step - loss: 0.6154 - acc: 0.6510\n",
      "Epoch 9/15\n",
      "768/768 [==============================] - 0s 96us/step - loss: 0.6097 - acc: 0.6510\n",
      "Epoch 10/15\n",
      "768/768 [==============================] - 0s 97us/step - loss: 0.6158 - acc: 0.6510\n",
      "Epoch 11/15\n",
      "768/768 [==============================] - 0s 98us/step - loss: 0.6051 - acc: 0.6510\n",
      "Epoch 12/15\n",
      "768/768 [==============================] - 0s 98us/step - loss: 0.6030 - acc: 0.6510\n",
      "Epoch 13/15\n",
      "768/768 [==============================] - 0s 97us/step - loss: 0.6003 - acc: 0.6510\n",
      "Epoch 14/15\n",
      "768/768 [==============================] - 0s 98us/step - loss: 0.6033 - acc: 0.6510\n",
      "Epoch 15/15\n",
      "768/768 [==============================] - 0s 96us/step - loss: 0.5993 - acc: 0.6510\n",
      "768/768 [==============================] - 0s 52us/step\n",
      "acc: 65.10%\n"
     ]
    }
   ],
   "source": [
    "# import requisite libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "\n",
    "# initialize random number generator\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "\n",
    "# split into input/predictor (X) and output/response (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "model.fit(X, Y, epochs=15, batch_size=10)\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X, Y)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Performance of Deep Learning Models\n",
    "\n",
    "Keras models are evaluated using:\n",
    "\n",
    "- Automatic verification dataset\n",
    "\n",
    "- Manual verification dataset\n",
    "\n",
    "- k-fold cross validation\n",
    "\n",
    "Other factors to consider are:\n",
    "\n",
    "- Network topology\n",
    "\n",
    "- Choice of loss function, activation functions, optimization procedure and\n",
    "number of epochs\n",
    "\n",
    "\n",
    "Training times are often long for deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptron (MLP) with Automatic Validation Set <a name=\"auto_validation\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 514 samples, validate on 254 samples\n",
      "Epoch 1/10\n",
      "514/514 [==============================] - 0s 623us/step - loss: 0.6796 - acc: 0.6401 - val_loss: 0.6584 - val_acc: 0.6732\n",
      "Epoch 2/10\n",
      "514/514 [==============================] - 0s 115us/step - loss: 0.6688 - acc: 0.6401 - val_loss: 0.6535 - val_acc: 0.6732\n",
      "Epoch 3/10\n",
      "514/514 [==============================] - 0s 112us/step - loss: 0.6615 - acc: 0.6381 - val_loss: 0.6510 - val_acc: 0.6732\n",
      "Epoch 4/10\n",
      "514/514 [==============================] - 0s 107us/step - loss: 0.6544 - acc: 0.6342 - val_loss: 0.6454 - val_acc: 0.6850\n",
      "Epoch 5/10\n",
      "514/514 [==============================] - 0s 113us/step - loss: 0.6451 - acc: 0.6401 - val_loss: 0.6350 - val_acc: 0.7047\n",
      "Epoch 6/10\n",
      "514/514 [==============================] - 0s 103us/step - loss: 0.6381 - acc: 0.6420 - val_loss: 0.6233 - val_acc: 0.6811\n",
      "Epoch 7/10\n",
      "514/514 [==============================] - 0s 101us/step - loss: 0.6306 - acc: 0.6576 - val_loss: 0.6147 - val_acc: 0.6890\n",
      "Epoch 8/10\n",
      "514/514 [==============================] - 0s 95us/step - loss: 0.6223 - acc: 0.6693 - val_loss: 0.6037 - val_acc: 0.6772\n",
      "Epoch 9/10\n",
      "514/514 [==============================] - 0s 96us/step - loss: 0.6182 - acc: 0.6615 - val_loss: 0.6065 - val_acc: 0.6890\n",
      "Epoch 10/10\n",
      "514/514 [==============================] - 0s 98us/step - loss: 0.6116 - acc: 0.6907 - val_loss: 0.5952 - val_acc: 0.6929\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c334fa748>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit the model - automatic validation set\n",
    "model.fit(X, Y, validation_split=0.33, epochs=10, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP with Manual Validation Set <a name=\"manual_validation\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 514 samples, validate on 254 samples\n",
      "Epoch 1/10\n",
      "514/514 [==============================] - 0s 694us/step - loss: 0.6793 - acc: 0.6576 - val_loss: 0.6748 - val_acc: 0.6378\n",
      "Epoch 2/10\n",
      "514/514 [==============================] - 0s 133us/step - loss: 0.6644 - acc: 0.6576 - val_loss: 0.6676 - val_acc: 0.6378\n",
      "Epoch 3/10\n",
      "514/514 [==============================] - 0s 124us/step - loss: 0.6586 - acc: 0.6576 - val_loss: 0.6596 - val_acc: 0.6378\n",
      "Epoch 4/10\n",
      "514/514 [==============================] - 0s 113us/step - loss: 0.6543 - acc: 0.6576 - val_loss: 0.6554 - val_acc: 0.6378\n",
      "Epoch 5/10\n",
      "514/514 [==============================] - 0s 114us/step - loss: 0.6516 - acc: 0.6595 - val_loss: 0.6615 - val_acc: 0.6378\n",
      "Epoch 6/10\n",
      "514/514 [==============================] - 0s 113us/step - loss: 0.6460 - acc: 0.6537 - val_loss: 0.6417 - val_acc: 0.6496\n",
      "Epoch 7/10\n",
      "514/514 [==============================] - 0s 112us/step - loss: 0.6368 - acc: 0.6693 - val_loss: 0.6546 - val_acc: 0.6378\n",
      "Epoch 8/10\n",
      "514/514 [==============================] - 0s 110us/step - loss: 0.6284 - acc: 0.6615 - val_loss: 0.6279 - val_acc: 0.6890\n",
      "Epoch 9/10\n",
      "514/514 [==============================] - 0s 115us/step - loss: 0.6178 - acc: 0.6693 - val_loss: 0.6171 - val_acc: 0.6811\n",
      "Epoch 10/10\n",
      "514/514 [==============================] - 0s 117us/step - loss: 0.6089 - acc: 0.6868 - val_loss: 0.6362 - val_acc: 0.6378\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c33b83748>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MLP with manual validation set\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy\n",
    "\n",
    "# random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "# manual validation - split into 67% for train and 33% for test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=seed)\n",
    "\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=10, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP with Manual k-Fold Cross Validation <a name=\"manual_kfold_validation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 64.94%\n",
      "acc: 70.13%\n",
      "acc: 66.23%\n",
      "acc: 79.22%\n",
      "acc: 72.73%\n",
      "acc: 66.23%\n",
      "acc: 55.84%\n",
      "acc: 57.14%\n",
      "acc: 67.11%\n",
      "acc: 64.47%\n",
      "66.40% (+/- 6.49%)\n"
     ]
    }
   ],
   "source": [
    "# MLP for Pima Indians Dataset with 10-fold cross validation\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy\n",
    "\n",
    "# random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "# manual specification - define 10-fold cross validation for evaluating the model\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "cvscores = []\n",
    "\n",
    "for train, test in kfold.split(X, Y):\n",
    "  # create the model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
    "    model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "    \n",
    "    # compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    # fit the model using the training data\n",
    "    model.fit(X[train], Y[train], epochs=10, batch_size=10, verbose=0)\n",
    "    \n",
    "    # evaluate the model using validation data\n",
    "    scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cvscores), numpy.std(cvscores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Deep Learning Keras Models with scikit-learn Library of Python\n",
    "\n",
    "- **Keras** is very much focused on deep learning and emphasizes minimalism.\n",
    "\n",
    "- **scikit-learn**, on the other hand, is a genearal purpose ML library and features several utilities (for example, resampling methods like k-fold cross validation and efficient search and evaluation of model hyperparameters.\n",
    "\n",
    "- Keras deep learning models can be used as classification or regression estimators in scikit-learn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions <a name=\"loss_optimiztion\"></a>\n",
    "\n",
    "- A measure of how well a neural network models your data.\n",
    "\n",
    "- If a prediction is way off from the expected, the loss function will indicate a higher value.\n",
    "\n",
    "\n",
    "\n",
    "- Regression Losses\n",
    "\n",
    "    - Mean Square Error/Quadratic Loss/L2 Loss\n",
    "\n",
    "    - Mean Absolute Error/L1 Loss\n",
    "\n",
    "    - Mean Bias Error\n",
    "\n",
    "- Classification Losses\n",
    "\n",
    "    - Hinge Loss/Multi class SVM Loss\n",
    "\n",
    "    - Cross Entropy Loss/Negative Log Likelihood\n",
    "    \n",
    "    \n",
    "\n",
    "[Loss functions](https://blog.algorithmia.com/introduction-to-loss-functions/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adam - an Optimization Algorithm\n",
    "\n",
    "- Adam is a replacement optimization algorithm for stochastic gradient descent\n",
    "\n",
    "- It can handle sparse gradients on noisy problems.\n",
    "\n",
    "\n",
    "\n",
    "[Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.653691046787343\n"
     ]
    }
   ],
   "source": [
    "# MLP for Pima Indians Dataset with 10-fold cross validation via sklearn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy\n",
    "\n",
    "# function to create a model, which is required for KerasClassifier\n",
    "def create_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
    "    model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load pima indians diabetes dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=10, verbose=0)\n",
    "\n",
    "# evaluate the model using 10-fold cross validation\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search Deep Learning Model Parameters <a name=\"grid_search\"></a>\n",
    "\n",
    "- Is the process of scanning the data to configure optimal parameters for a given model.\n",
    "\n",
    "- In other words, it is a brute force approach for estimating a model's **hyperparameters**.\n",
    "\n",
    "- A model's hyperparameter is a configuration that is external to the model.\n",
    "\n",
    "- Hyperparameters cannot be estimated from data.\n",
    "\n",
    "- Grid search iterates through every parameter combination and stores a model for each combination.\n",
    "\n",
    "- Grid-searching can be extremely computationally expensive and may take quite a long time to run.\n",
    "\n",
    "- If we have $k$ hyperparameters, and each one has $c_i$ possible values, grid search will explore $\\prod_{i=1}^{k}c_i$.\n",
    "\n",
    "Optimizers for searching diffeerent weight values:\n",
    "\n",
    "- Number of epochs for training the model for diffeerent number of exposures to the training\n",
    "dataset. \n",
    "\n",
    "- Batches for varying the number of samples before weight updates.\n",
    "\n",
    "- (2 x 3 x 3 x 3) combinations of optimizers, initializations, epochs and batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP for Pima Indians Dataset with grid search via sklearn\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy\n",
    "\n",
    "# Function to create model, required for KerasClassifier\n",
    "def create_model(optimizer='rmsprop', init='glorot_uniform'):\n",
    "    # create the model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dense(8, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n",
    "    # compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "# create the model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# grid search epochs, batch size and optimizer\n",
    "optimizers = ['rmsprop', 'adam']\n",
    "init = ['glorot_uniform', 'normal', 'uniform']\n",
    "\n",
    "epochs = numpy.array([50, 100, 150])\n",
    "batches = numpy.array([5, 10, 20])\n",
    "\n",
    "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "grid_result = grid.fit(X, Y)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "for params, mean_score, scores in grid_result.grid_scores_:\n",
    "    print(\"%f (%f) with: %r\" % (scores.mean(), scores.std(), params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Problems <a name=\"classification_problems\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification with the Iris Flowers Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 44.00% (17.44%)\n"
     ]
    }
   ],
   "source": [
    "# multiclass classification with the Iris Flowers Dataset\n",
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"iris.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "X = dataset[:,0:4].astype(float)\n",
    "Y = dataset[:,4]\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "\n",
    "# convert integers to dummy variables (i.e., one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "# define baseline model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(4, input_dim=4, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(3, kernel_initializer='normal', activation='sigmoid'))\n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=10, batch_size=5, verbose=0)\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "results = cross_val_score(estimator, X, dummy_y, cv=kfold)\n",
    "\n",
    "print(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sonar Object Classification Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 75.92% (6.93%)\n"
     ]
    }
   ],
   "source": [
    "# binary classification with Sonar dataset: baseline\n",
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"sonar.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "\n",
    "# split data into input (X) and output (Y) variables\n",
    "X = dataset[:,0:60].astype(float)\n",
    "Y = dataset[:,60]\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "\n",
    "# baseline model\n",
    "def create_baseline():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    # compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# evaluate model with standardized dataset\n",
    "estimator = KerasClassifier(build_fn=create_baseline, epochs=10, batch_size=5, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "results = cross_val_score(estimator, X, encoded_Y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Model Performance with Data Preparation <a name=\"improving_model_performance\"></a>\n",
    "\n",
    "- Data standardization using the StandardScaler class.\n",
    "\n",
    "- Good practice to train the standardization procedure on the training data within the pass of a CV run; use the trained standardization instance to prepare the unseen test fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized: 83.61% (6.63%)\n"
     ]
    }
   ],
   "source": [
    "# binary classification with Sonar dataset: Standardized\n",
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"sonar.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "\n",
    "# split data into input (X) and target (Y) variables\n",
    "X = dataset[:,0:60].astype(float)\n",
    "Y = dataset[:,60]\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "\n",
    "# baseline model\n",
    "def create_baseline():\n",
    "    # create the model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    \n",
    "    # compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "# evaluate baseline model with standardized dataset\n",
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=10, batch_size=5, verbose=0)))\n",
    "\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "print(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Layers and Neurons\n",
    "\n",
    "- Two experiments on the structure of the network: making it smaller and making it larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smaller: 80.26% (7.68%)\n"
     ]
    }
   ],
   "source": [
    "# binary classification with Sonar dataset: standardized smaller\n",
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"sonar.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:60].astype(float)\n",
    "Y = dataset[:,60]\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "\n",
    "# smaller model\n",
    "def create_smaller():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(30, input_dim=60, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_smaller, epochs=5, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "print(\"Smaller: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larger: 84.54% (6.89%)\n"
     ]
    }
   ],
   "source": [
    "# binary classification with Sonar dataset: standardized larger\n",
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"sonar.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "\n",
    "# split data into input (X) and target (Y) variables\n",
    "X = dataset[:,0:60].astype(float)\n",
    "Y = dataset[:,60]\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "\n",
    "# larger model\n",
    "def create_larger():\n",
    "    # create the model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(30, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    # compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_larger, epochs=10, batch_size=5, verbose=0)))\n",
    "\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "print(\"Larger: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Problems <a name=\"iregression_problems\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression of Boston House Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: -57.84 (42.32) MSE\n"
     ]
    }
   ],
   "source": [
    "# regression example with Boston Dataset: Baseline\n",
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"housing.csv\", delim_whitespace=True, header=None)\n",
    "dataset = dataframe.values\n",
    "\n",
    "# split data into input (X) and target (Y) variables\n",
    "X = dataset[:,0:13]\n",
    "Y = dataset[:,13]\n",
    "\n",
    "# define the base model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# evaluate model with standardized dataset\n",
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=10, batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "\n",
    "results = cross_val_score(estimator, X, Y, cv=kfold)\n",
    "print(\"Baseline: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized: -59.64 (48.61) MSE\n"
     ]
    }
   ],
   "source": [
    "# regression example with the Boston dataset: standardized\n",
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"housing.csv\", delim_whitespace=True, header=None)\n",
    "dataset = dataframe.values\n",
    "\n",
    "# split into input (X) and target (Y) variables\n",
    "X = dataset[:,0:13]\n",
    "Y = dataset[:,13]\n",
    "\n",
    "# define the base model\n",
    "def baseline_model():\n",
    "    # create the model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # compile the model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# evaluate the model with standardized dataset\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=baseline_model, epochs=10, batch_size=5, verbose=0)))\n",
    "\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
    "print(\"Standardized: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larger: -42.78 (46.51) MSE\n"
     ]
    }
   ],
   "source": [
    "# regression example with Boston Dataset: standardized and larger\n",
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"housing.csv\", delim_whitespace=True, header=None)\n",
    "dataset = dataframe.values\n",
    "\n",
    "# split data into input (X) and target (Y) variables\n",
    "X = dataset[:,0:13]\n",
    "Y = dataset[:,13]\n",
    "\n",
    "# define the model\n",
    "def larger_model():\n",
    "    # create the model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(6, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # compile the model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# evaluate the model with standardized dataset\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=larger_model, epochs=10, batch_size=5, verbose=0)))\n",
    "\n",
    "pipeline = Pipeline(estimators)\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
    "\n",
    "print(\"Larger: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wider: -47.72 (42.41) MSE\n"
     ]
    }
   ],
   "source": [
    "# regression example with Boston Dataset: Standardized and Wider\n",
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"housing.csv\", delim_whitespace=True, header=None)\n",
    "dataset = dataframe.values\n",
    "\n",
    "# split data into input (X) and target (Y) variables\n",
    "X = dataset[:,0:13]\n",
    "Y = dataset[:,13]\n",
    "\n",
    "# define a wider model\n",
    "def wider_model():\n",
    "    # create the model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # compile the model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# evaluate the model with standardized dataset\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=wider_model, epochs=10, batch_size=5, verbose=0)))\n",
    "\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "\n",
    "results = cross_val_score(pipeline, X, Y, cv=kfold)\n",
    "print(\"Wider: %.2f (%.2f) MSE\" % (results.mean(), results.std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving ML Models in HDF5, JSON, and YAML Formats with Serialization <a name=\"saving_models\"></a>\n",
    "\n",
    "\n",
    "- Keras saves **model weights** in Hierarchical Data Format (HDF5), which is a grid format suitable for storing multi-dimensional arrays of numbers.\n",
    "\n",
    "- Model **architecture** is stored/restored in JSON and YAML formats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install Python support for the HDF5 file format\n",
    "\n",
    "sudo pip install h5py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Neural Network Model to JSON Format\n",
    "\n",
    "- to_json(), model_from_json(), save_weights(), and load_weights() functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 65.10%\n",
      "Saved model to disk\n",
      "Loaded model from disk\n",
      "acc: 65.10\n"
     ]
    }
   ],
   "source": [
    "# MLP for Pima Indians Dataset serialize to JSON and HDF5\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import model_from_json\n",
    "import numpy\n",
    "import os\n",
    "\n",
    "# random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "\n",
    "# split data into input (X) and target (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "model.fit(X, Y, epochs=10, batch_size=10, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X, Y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "# subsequently ...\n",
    "\n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X, Y, verbose=0)\n",
    "\n",
    "print (\"%s: %.2f\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 76.43%\n",
      "Saved model to disk\n",
      "Loaded model from disk\n",
      "acc: 76.43%\n"
     ]
    }
   ],
   "source": [
    "# MLP for Pima Indians dataset serialize to YAML and HDF5\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import model_from_yaml\n",
    "import numpy\n",
    "import os\n",
    "\n",
    "# random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "model.fit(X, Y, epochs=150, batch_size=10, verbose=0)\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X, Y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# serialize model to YAML\n",
    "model_yaml = model.to_yaml()\n",
    "with open(\"model.yaml\", \"w\") as yaml_file:\n",
    "    yaml_file.write(model_yaml)\n",
    "\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "# subsequently ...\n",
    "\n",
    "# load YAML and create model\n",
    "yaml_file = open('model.yaml', 'r')\n",
    "loaded_model_yaml = yaml_file.read()\n",
    "yaml_file.close()\n",
    "loaded_model = model_from_yaml(loaded_model_yaml)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X, Y, verbose=0)\n",
    "\n",
    "print (\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpointing Models During Training <a name=\"check_pointing\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To guard aganist system failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.67323, saving model to weights.best.hdf5\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.67323\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.67323\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.67323 to 0.68504, saving model to weights.best.hdf5\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.68504 to 0.68898, saving model to weights.best.hdf5\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.68898 to 0.69685, saving model to weights.best.hdf5\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.69685\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.69685\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.69685\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.69685\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c3633f898>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpoint the weights for best model on validation accuracy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "# random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "\n",
    "# split data into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# checkpoint\n",
    "filepath=\"weights.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# fit the model\n",
    "model.fit(X, Y, validation_split=0.33, epochs=10, batch_size=10, callbacks=callbacks_list, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created model and loaded weights from file\n",
      "acc: 68.10%\n"
     ]
    }
   ],
   "source": [
    "# load and use weights from a checkpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "# random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\n",
    "# load weights\n",
    "model.load_weights(\"weights.best.hdf5\")\n",
    "\n",
    "# compile model (required to make predictions)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(\"Created model and loaded weights from file\")\n",
    "\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "# estimate accuracy on the whole dataset using loaded weights\n",
    "scores = model.evaluate(X, Y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.67323, saving model to weights-improvement-01-0.67.hdf5\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.67323\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.67323\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.67323 to 0.67717, saving model to weights-improvement-04-0.68.hdf5\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.67717 to 0.69291, saving model to weights-improvement-05-0.69.hdf5\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.69291 to 0.69685, saving model to weights-improvement-06-0.70.hdf5\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.69685\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.69685\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.69685\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.69685\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c367580f0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpoint the weights when validation accuracy improves\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "# random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# fit the model\n",
    "model.fit(X, Y, validation_split=0.33, epochs=10, batch_size=10, callbacks=callbacks_list, verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize training history\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "# random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(X, Y, validation_split=0.33, epochs=10, batch_size=10, verbose=0)\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Models <a name=\"improving_models\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce Overfitting With Dropout Regularization <a name=\"dropout_regularization\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Dropout** is a technique where randomly selected neurons are ignored during training -- dropped-out randomly. \n",
    "\n",
    "- Contribution of dropped neurons to the activation of downstream neurons is temporally removed on the forward pass. \n",
    "\n",
    "- Also, any weight updates are not applied to the (dropped) neurons on the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 86.97% (8.99%)\n"
     ]
    }
   ],
   "source": [
    "# baseline model on the Sonar dataset\n",
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"sonar.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "\n",
    "# split data into input (X) and target (Y) variables\n",
    "X = dataset[:,0:60].astype(float)\n",
    "Y = dataset[:,60]\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "\n",
    "# baseline\n",
    "def create_baseline():\n",
    "    # create the model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(30, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    # compile the model\n",
    "    sgd = SGD(lr=0.01, momentum=0.8, decay=0.0, nesterov=False)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=16, verbose=0)))\n",
    "\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:37: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(60, input_dim=60, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:37: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(60, input_dim=60, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:37: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(60, input_dim=60, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:37: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(60, input_dim=60, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:37: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(60, input_dim=60, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:37: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(60, input_dim=60, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:37: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(60, input_dim=60, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:37: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(60, input_dim=60, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:37: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(60, input_dim=60, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:37: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(60, input_dim=60, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden: 79.28% (5.90%)\n"
     ]
    }
   ],
   "source": [
    "# example of dropout on the Sonar dataset: hidden layer\n",
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"sonar.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "\n",
    "# split data into input (X) and target (Y) variables\n",
    "X = dataset[:,0:60].astype(float)\n",
    "Y = dataset[:,60]\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "\n",
    "# dropout in hidden layers with weight constraint\n",
    "def create_model():\n",
    "    # create the model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(60, input_dim=60, kernel_initializer='normal', activation='relu', W_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(30, kernel_initializer='normal', activation='relu', W_constraint=maxnorm(3)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    # compile the model\n",
    "    sgd = SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=False)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_model, epochs=10, batch_size=16, verbose=0)))\n",
    "\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "print(\"Hidden: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(60, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(60, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(60, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(60, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(60, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(60, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(60, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(60, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(60, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(60, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, kernel_initializer=\"normal\", activation=\"relu\", kernel_constraint=<keras.con...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visible: 83.61% (6.52%)\n"
     ]
    }
   ],
   "source": [
    "# Example of Dropout on the Sonar dataset: visible layer\n",
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"sonar.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "\n",
    "# split data into input (X) and target (Y) variables\n",
    "X = dataset[:,0:60].astype(float)\n",
    "Y = dataset[:,60]\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "\n",
    "# dropout in the input layer with weight constraint\n",
    "def create_model():\n",
    "    # create the model\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0.2, input_shape=(60,)))\n",
    "    model.add(Dense(60, kernel_initializer='normal', activation='relu', W_constraint=maxnorm(3)))\n",
    "    model.add(Dense(30, kernel_initializer='normal', activation='relu', W_constraint=maxnorm(3)))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    # compile the model\n",
    "    sgd = SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=False)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "numpy.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_model, epochs=10, batch_size=16, verbose=0)))\n",
    "\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "\n",
    "results = cross_val_score(pipeline, X, encoded_Y, cv=kfold)\n",
    "print(\"Visible: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Performance With Learning Rate Schedules <a name=\"learning_rate_schedules\"></a>\n",
    "\n",
    "\n",
    "- Achieve increased performance and faster training on some problems by using a learning rate that changes during training.\n",
    "\n",
    "- We will use different learning rate schedules for your neural network models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 235 samples, validate on 116 samples\n",
      "Epoch 1/50\n",
      " - 2s - loss: 0.6803 - acc: 0.6468 - val_loss: 0.6192 - val_acc: 0.9138\n",
      "Epoch 2/50\n",
      " - 0s - loss: 0.6193 - acc: 0.7234 - val_loss: 0.4764 - val_acc: 0.8621\n",
      "Epoch 3/50\n",
      " - 0s - loss: 0.4980 - acc: 0.8255 - val_loss: 0.3682 - val_acc: 0.9397\n",
      "Epoch 4/50\n",
      " - 0s - loss: 0.3622 - acc: 0.8596 - val_loss: 0.3808 - val_acc: 0.8793\n",
      "Epoch 5/50\n",
      " - 0s - loss: 0.2791 - acc: 0.8851 - val_loss: 0.1572 - val_acc: 0.9655\n",
      "Epoch 6/50\n",
      " - 0s - loss: 0.2150 - acc: 0.9234 - val_loss: 0.2224 - val_acc: 0.9224\n",
      "Epoch 7/50\n",
      " - 0s - loss: 0.1791 - acc: 0.9362 - val_loss: 0.1508 - val_acc: 0.9655\n",
      "Epoch 8/50\n",
      " - 0s - loss: 0.1582 - acc: 0.9362 - val_loss: 0.0869 - val_acc: 0.9655\n",
      "Epoch 9/50\n",
      " - 0s - loss: 0.1756 - acc: 0.9404 - val_loss: 0.1410 - val_acc: 0.9741\n",
      "Epoch 10/50\n",
      " - 0s - loss: 0.1261 - acc: 0.9660 - val_loss: 0.1058 - val_acc: 0.9914\n",
      "Epoch 11/50\n",
      " - 0s - loss: 0.1170 - acc: 0.9574 - val_loss: 0.0924 - val_acc: 0.9914\n",
      "Epoch 12/50\n",
      " - 0s - loss: 0.1060 - acc: 0.9702 - val_loss: 0.0869 - val_acc: 0.9914\n",
      "Epoch 13/50\n",
      " - 0s - loss: 0.1072 - acc: 0.9617 - val_loss: 0.0960 - val_acc: 0.9914\n",
      "Epoch 14/50\n",
      " - 0s - loss: 0.0988 - acc: 0.9745 - val_loss: 0.0886 - val_acc: 0.9828\n",
      "Epoch 15/50\n",
      " - 0s - loss: 0.0949 - acc: 0.9702 - val_loss: 0.0733 - val_acc: 0.9914\n",
      "Epoch 16/50\n",
      " - 0s - loss: 0.0996 - acc: 0.9660 - val_loss: 0.0925 - val_acc: 0.9914\n",
      "Epoch 17/50\n",
      " - 0s - loss: 0.0871 - acc: 0.9702 - val_loss: 0.0839 - val_acc: 0.9914\n",
      "Epoch 18/50\n",
      " - 0s - loss: 0.0821 - acc: 0.9787 - val_loss: 0.0814 - val_acc: 0.9914\n",
      "Epoch 19/50\n",
      " - 0s - loss: 0.0778 - acc: 0.9745 - val_loss: 0.0790 - val_acc: 0.9914\n",
      "Epoch 20/50\n",
      " - 0s - loss: 0.0763 - acc: 0.9787 - val_loss: 0.0636 - val_acc: 0.9914\n",
      "Epoch 21/50\n",
      " - 0s - loss: 0.0752 - acc: 0.9787 - val_loss: 0.0833 - val_acc: 0.9914\n",
      "Epoch 22/50\n",
      " - 0s - loss: 0.0711 - acc: 0.9830 - val_loss: 0.0731 - val_acc: 0.9914\n",
      "Epoch 23/50\n",
      " - 0s - loss: 0.0707 - acc: 0.9787 - val_loss: 0.0684 - val_acc: 0.9914\n",
      "Epoch 24/50\n",
      " - 0s - loss: 0.0691 - acc: 0.9830 - val_loss: 0.0803 - val_acc: 0.9914\n",
      "Epoch 25/50\n",
      " - 0s - loss: 0.0668 - acc: 0.9830 - val_loss: 0.0665 - val_acc: 0.9914\n",
      "Epoch 26/50\n",
      " - 0s - loss: 0.0664 - acc: 0.9830 - val_loss: 0.0753 - val_acc: 0.9914\n",
      "Epoch 27/50\n",
      " - 0s - loss: 0.0633 - acc: 0.9830 - val_loss: 0.0705 - val_acc: 0.9914\n",
      "Epoch 28/50\n",
      " - 0s - loss: 0.0635 - acc: 0.9830 - val_loss: 0.0704 - val_acc: 0.9914\n",
      "Epoch 29/50\n",
      " - 0s - loss: 0.0618 - acc: 0.9830 - val_loss: 0.0722 - val_acc: 0.9914\n",
      "Epoch 30/50\n",
      " - 0s - loss: 0.0608 - acc: 0.9830 - val_loss: 0.0671 - val_acc: 0.9914\n",
      "Epoch 31/50\n",
      " - 0s - loss: 0.0602 - acc: 0.9830 - val_loss: 0.0717 - val_acc: 0.9914\n",
      "Epoch 32/50\n",
      " - 0s - loss: 0.0593 - acc: 0.9830 - val_loss: 0.0704 - val_acc: 0.9914\n",
      "Epoch 33/50\n",
      " - 0s - loss: 0.0586 - acc: 0.9830 - val_loss: 0.0676 - val_acc: 0.9914\n",
      "Epoch 34/50\n",
      " - 0s - loss: 0.0586 - acc: 0.9830 - val_loss: 0.0690 - val_acc: 0.9914\n",
      "Epoch 35/50\n",
      " - 0s - loss: 0.0584 - acc: 0.9830 - val_loss: 0.0738 - val_acc: 0.9914\n",
      "Epoch 36/50\n",
      " - 0s - loss: 0.0571 - acc: 0.9872 - val_loss: 0.0692 - val_acc: 0.9914\n",
      "Epoch 37/50\n",
      " - 0s - loss: 0.0568 - acc: 0.9830 - val_loss: 0.0662 - val_acc: 0.9914\n",
      "Epoch 38/50\n",
      " - 0s - loss: 0.0569 - acc: 0.9830 - val_loss: 0.0689 - val_acc: 0.9914\n",
      "Epoch 39/50\n",
      " - 0s - loss: 0.0563 - acc: 0.9830 - val_loss: 0.0637 - val_acc: 0.9914\n",
      "Epoch 40/50\n",
      " - 0s - loss: 0.0556 - acc: 0.9830 - val_loss: 0.0656 - val_acc: 0.9914\n",
      "Epoch 41/50\n",
      " - 0s - loss: 0.0549 - acc: 0.9872 - val_loss: 0.0699 - val_acc: 0.9914\n",
      "Epoch 42/50\n",
      " - 0s - loss: 0.0550 - acc: 0.9872 - val_loss: 0.0682 - val_acc: 0.9914\n",
      "Epoch 43/50\n",
      " - 0s - loss: 0.0548 - acc: 0.9872 - val_loss: 0.0667 - val_acc: 0.9914\n",
      "Epoch 44/50\n",
      " - 0s - loss: 0.0547 - acc: 0.9872 - val_loss: 0.0667 - val_acc: 0.9914\n",
      "Epoch 45/50\n",
      " - 0s - loss: 0.0546 - acc: 0.9830 - val_loss: 0.0643 - val_acc: 0.9914\n",
      "Epoch 46/50\n",
      " - 0s - loss: 0.0544 - acc: 0.9830 - val_loss: 0.0646 - val_acc: 0.9914\n",
      "Epoch 47/50\n",
      " - 0s - loss: 0.0550 - acc: 0.9872 - val_loss: 0.0696 - val_acc: 0.9914\n",
      "Epoch 48/50\n",
      " - 0s - loss: 0.0538 - acc: 0.9872 - val_loss: 0.0682 - val_acc: 0.9914\n",
      "Epoch 49/50\n",
      " - 0s - loss: 0.0537 - acc: 0.9872 - val_loss: 0.0648 - val_acc: 0.9914\n",
      "Epoch 50/50\n",
      " - 0s - loss: 0.0535 - acc: 0.9872 - val_loss: 0.0679 - val_acc: 0.9914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c39358780>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop-based learning rate decay\n",
    "import pandas\n",
    "import pandas\n",
    "import numpy\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "# learning rate schedule\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "\n",
    "# random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"ionosphere.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "\n",
    "# split data into input (X) and target (Y) variables\n",
    "X = dataset[:,0:34].astype(float)\n",
    "Y = dataset[:,34]\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "Y = encoder.transform(Y)\n",
    "\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(34, input_dim=34, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "sgd = SGD(lr=0.0, momentum=0.9, decay=0.0, nesterov=False)\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# learning schedule callback\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "callbacks_list = [lrate]\n",
    "\n",
    "# fit the model\n",
    "model.fit(X, Y, validation_split=0.33, epochs=50, batch_size=28, callbacks=callbacks_list, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 235 samples, validate on 116 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 0.6813 - acc: 0.6468 - val_loss: 0.6378 - val_acc: 0.8621\n",
      "Epoch 2/50\n",
      " - 0s - loss: 0.6369 - acc: 0.7319 - val_loss: 0.5289 - val_acc: 0.8276\n",
      "Epoch 3/50\n",
      " - 0s - loss: 0.5584 - acc: 0.8213 - val_loss: 0.4759 - val_acc: 0.8362\n",
      "Epoch 4/50\n",
      " - 0s - loss: 0.4680 - acc: 0.8383 - val_loss: 0.4409 - val_acc: 0.9224\n",
      "Epoch 5/50\n",
      " - 0s - loss: 0.3843 - acc: 0.8681 - val_loss: 0.2791 - val_acc: 0.9483\n",
      "Epoch 6/50\n",
      " - 0s - loss: 0.3166 - acc: 0.8809 - val_loss: 0.3944 - val_acc: 0.8879\n",
      "Epoch 7/50\n",
      " - 0s - loss: 0.2765 - acc: 0.9064 - val_loss: 0.2301 - val_acc: 0.9483\n",
      "Epoch 8/50\n",
      " - 0s - loss: 0.2413 - acc: 0.9106 - val_loss: 0.1443 - val_acc: 0.9569\n",
      "Epoch 9/50\n",
      " - 0s - loss: 0.2443 - acc: 0.9106 - val_loss: 0.2169 - val_acc: 0.9483\n",
      "Epoch 10/50\n",
      " - 0s - loss: 0.2021 - acc: 0.9149 - val_loss: 0.2562 - val_acc: 0.9224\n",
      "Epoch 11/50\n",
      " - 0s - loss: 0.1924 - acc: 0.9234 - val_loss: 0.1907 - val_acc: 0.9483\n",
      "Epoch 12/50\n",
      " - 0s - loss: 0.1732 - acc: 0.9404 - val_loss: 0.1129 - val_acc: 0.9655\n",
      "Epoch 13/50\n",
      " - 0s - loss: 0.1802 - acc: 0.9277 - val_loss: 0.1029 - val_acc: 0.9741\n",
      "Epoch 14/50\n",
      " - 0s - loss: 0.1674 - acc: 0.9362 - val_loss: 0.1693 - val_acc: 0.9483\n",
      "Epoch 15/50\n",
      " - 0s - loss: 0.1435 - acc: 0.9532 - val_loss: 0.0968 - val_acc: 0.9828\n",
      "Epoch 16/50\n",
      " - 0s - loss: 0.1521 - acc: 0.9447 - val_loss: 0.1770 - val_acc: 0.9483\n",
      "Epoch 17/50\n",
      " - 0s - loss: 0.1475 - acc: 0.9489 - val_loss: 0.1460 - val_acc: 0.9655\n",
      "Epoch 18/50\n",
      " - 0s - loss: 0.1358 - acc: 0.9489 - val_loss: 0.1203 - val_acc: 0.9828\n",
      "Epoch 19/50\n",
      " - 0s - loss: 0.1277 - acc: 0.9489 - val_loss: 0.0925 - val_acc: 0.9914\n",
      "Epoch 20/50\n",
      " - 0s - loss: 0.1215 - acc: 0.9660 - val_loss: 0.1116 - val_acc: 0.9914\n",
      "Epoch 21/50\n",
      " - 0s - loss: 0.1158 - acc: 0.9574 - val_loss: 0.1046 - val_acc: 0.9914\n",
      "Epoch 22/50\n",
      " - 0s - loss: 0.1091 - acc: 0.9574 - val_loss: 0.1078 - val_acc: 0.9914\n",
      "Epoch 23/50\n",
      " - 0s - loss: 0.1093 - acc: 0.9617 - val_loss: 0.1059 - val_acc: 0.9914\n",
      "Epoch 24/50\n",
      " - 0s - loss: 0.1016 - acc: 0.9660 - val_loss: 0.0756 - val_acc: 0.9914\n",
      "Epoch 25/50\n",
      " - 0s - loss: 0.1105 - acc: 0.9617 - val_loss: 0.1100 - val_acc: 0.9914\n",
      "Epoch 26/50\n",
      " - 0s - loss: 0.0963 - acc: 0.9617 - val_loss: 0.0843 - val_acc: 0.9914\n",
      "Epoch 27/50\n",
      " - 0s - loss: 0.0958 - acc: 0.9702 - val_loss: 0.0886 - val_acc: 0.9914\n",
      "Epoch 28/50\n",
      " - 0s - loss: 0.0938 - acc: 0.9702 - val_loss: 0.0893 - val_acc: 0.9914\n",
      "Epoch 29/50\n",
      " - 0s - loss: 0.0855 - acc: 0.9787 - val_loss: 0.0876 - val_acc: 0.9914\n",
      "Epoch 30/50\n",
      " - 0s - loss: 0.0874 - acc: 0.9745 - val_loss: 0.0887 - val_acc: 0.9914\n",
      "Epoch 31/50\n",
      " - 0s - loss: 0.0851 - acc: 0.9745 - val_loss: 0.0832 - val_acc: 0.9914\n",
      "Epoch 32/50\n",
      " - 0s - loss: 0.0816 - acc: 0.9830 - val_loss: 0.0849 - val_acc: 0.9914\n",
      "Epoch 33/50\n",
      " - 0s - loss: 0.0772 - acc: 0.9830 - val_loss: 0.0865 - val_acc: 0.9914\n",
      "Epoch 34/50\n",
      " - 0s - loss: 0.0802 - acc: 0.9787 - val_loss: 0.0950 - val_acc: 0.9914\n",
      "Epoch 35/50\n",
      " - 0s - loss: 0.0753 - acc: 0.9830 - val_loss: 0.0743 - val_acc: 0.9828\n",
      "Epoch 36/50\n",
      " - 0s - loss: 0.0761 - acc: 0.9830 - val_loss: 0.0780 - val_acc: 0.9828\n",
      "Epoch 37/50\n",
      " - 0s - loss: 0.0709 - acc: 0.9830 - val_loss: 0.0854 - val_acc: 0.9914\n",
      "Epoch 38/50\n",
      " - 0s - loss: 0.0717 - acc: 0.9787 - val_loss: 0.0718 - val_acc: 0.9914\n",
      "Epoch 39/50\n",
      " - 0s - loss: 0.0748 - acc: 0.9787 - val_loss: 0.0864 - val_acc: 0.9914\n",
      "Epoch 40/50\n",
      " - 0s - loss: 0.0699 - acc: 0.9830 - val_loss: 0.0726 - val_acc: 0.9914\n",
      "Epoch 41/50\n",
      " - 0s - loss: 0.0657 - acc: 0.9787 - val_loss: 0.0828 - val_acc: 0.9914\n",
      "Epoch 42/50\n",
      " - 0s - loss: 0.0667 - acc: 0.9787 - val_loss: 0.0845 - val_acc: 0.9914\n",
      "Epoch 43/50\n",
      " - 0s - loss: 0.0663 - acc: 0.9830 - val_loss: 0.0743 - val_acc: 0.9914\n",
      "Epoch 44/50\n",
      " - 0s - loss: 0.0662 - acc: 0.9787 - val_loss: 0.0673 - val_acc: 0.9828\n",
      "Epoch 45/50\n",
      " - 0s - loss: 0.0625 - acc: 0.9830 - val_loss: 0.0906 - val_acc: 0.9914\n",
      "Epoch 46/50\n",
      " - 0s - loss: 0.0685 - acc: 0.9830 - val_loss: 0.0697 - val_acc: 0.9914\n",
      "Epoch 47/50\n",
      " - 0s - loss: 0.0665 - acc: 0.9872 - val_loss: 0.0612 - val_acc: 0.9828\n",
      "Epoch 48/50\n",
      " - 0s - loss: 0.0627 - acc: 0.9872 - val_loss: 0.0795 - val_acc: 0.9914\n",
      "Epoch 49/50\n",
      " - 0s - loss: 0.0592 - acc: 0.9830 - val_loss: 0.0767 - val_acc: 0.9914\n",
      "Epoch 50/50\n",
      " - 0s - loss: 0.0598 - acc: 0.9830 - val_loss: 0.0619 - val_acc: 0.9828\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c41218c50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time-based learning rate decay\n",
    "import pandas\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load dataset\n",
    "dataframe = pandas.read_csv(\"ionosphere.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "\n",
    "# split data into input (X) and target (Y) variables\n",
    "X = dataset[:,0:34].astype(float)\n",
    "Y = dataset[:,34]\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "Y = encoder.transform(Y)\n",
    "\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(34, input_dim=34, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "num_epochs = 50\n",
    "learning_rate = 0.1\n",
    "decay_rate = learning_rate / num_epochs\n",
    "momentum = 0.8\n",
    "\n",
    "sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "# fit the model\n",
    "model.fit(X, Y, validation_split=0.33, epochs=num_epochs, batch_size=28, verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (CNNs)<a name=\"cnns\"></a>\n",
    "\n",
    "\n",
    "- Suitable for solving difficult computer vision and natural language processing tasks.\n",
    "\n",
    "- Building blocks: convolutional layers, pool layers, and fully-connected layers.\n",
    "\n",
    "- CNNs use fewer parameters (weights) to learn than a fully connected network.\n",
    "\n",
    "- Invariant to object position and distortion in the scene.\n",
    "\n",
    "- Automatically learn and generalize features from the input domain.\n",
    "\n",
    "### Convolutional Layers\n",
    "\n",
    "- Comprised of filters and feature maps.\n",
    "\n",
    "### Pooling Layers\n",
    "\n",
    "- Down-sample the previous layers feature map.\n",
    "\n",
    "### Fully Connected Layers\n",
    "\n",
    "- Normal flat feedforward neural network layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 5s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), input_shape=(1, 28, 28..., activation=\"relu\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      " - 65s - loss: 0.2226 - acc: 0.9365 - val_loss: 0.0796 - val_acc: 0.9747\n",
      "Epoch 2/10\n",
      " - 62s - loss: 0.0712 - acc: 0.9786 - val_loss: 0.0452 - val_acc: 0.9846\n",
      "Epoch 3/10\n",
      " - 398s - loss: 0.0513 - acc: 0.9842 - val_loss: 0.0438 - val_acc: 0.9858\n",
      "Epoch 4/10\n",
      " - 66s - loss: 0.0390 - acc: 0.9879 - val_loss: 0.0416 - val_acc: 0.9873\n",
      "Epoch 5/10\n",
      " - 65s - loss: 0.0324 - acc: 0.9898 - val_loss: 0.0357 - val_acc: 0.9881\n",
      "Epoch 6/10\n",
      " - 63s - loss: 0.0268 - acc: 0.9918 - val_loss: 0.0331 - val_acc: 0.9894\n",
      "Epoch 7/10\n",
      " - 62s - loss: 0.0220 - acc: 0.9928 - val_loss: 0.0357 - val_acc: 0.9877\n",
      "Epoch 8/10\n",
      " - 62s - loss: 0.0190 - acc: 0.9941 - val_loss: 0.0349 - val_acc: 0.9885\n",
      "Epoch 9/10\n",
      " - 61s - loss: 0.0158 - acc: 0.9948 - val_loss: 0.0331 - val_acc: 0.9888\n",
      "Epoch 10/10\n",
      " - 64s - loss: 0.0144 - acc: 0.9955 - val_loss: 0.0339 - val_acc: 0.9891\n",
      "CNN Error: 1.09%\n"
     ]
    }
   ],
   "source": [
    "# simple CNN for the MNIST dataset\n",
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "# random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# reshape to be [samples][channels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')\n",
    "\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "\n",
    "# define a simple CNN model\n",
    "def baseline_model():\n",
    "    # create the model\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, 5, 5, input_shape=(1, 28, 28), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# build the model\n",
    "model = baseline_model()\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
    "\n",
    "# final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# larger CNN for the MNIST dataset\n",
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "# random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')\n",
    "\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "\n",
    "# define the larger model\n",
    "def larger_model():\n",
    "    # create the model\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(30, 5, 5, input_shape=(1, 28, 28), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Convolution2D(15, 3, 3, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    # compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# build the model\n",
    "model = larger_model()\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
    "\n",
    "# final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Large CNN Error: %.2f%%\" % (100-scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline MLP for MNIST dataset\n",
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# flatten 28*28 images to a 784 vector for each image\n",
    "num_pixels = X_train.shape[1] * X_train.shape[2]\n",
    "X_train = X_train.reshape(X_train.shape[0], num_pixels).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], num_pixels).astype('float32')\n",
    "\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "\n",
    "# define baseline model\n",
    "def baseline_model():\n",
    "    # create the model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_pixels, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
    "    # compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# build the model\n",
    "model = baseline_model()\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=200, verbose=2)\n",
    "\n",
    "# final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ad hoc mnist instances\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load (downloaded if needed) the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# plot 4 images as gray scale\n",
    "plt.subplot(221)\n",
    "plt.imshow(X_train[0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(222)\n",
    "plt.imshow(X_train[1], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(223)\n",
    "plt.imshow(X_train[2], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(224)\n",
    "plt.imshow(X_train[3], cmap=plt.get_cmap('gray'))\n",
    "\n",
    "# show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Model Performance With Image Augmentation\n",
    "\n",
    "- Data augmentation is also required for more complex object recognition tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of images as baseline for comparison\n",
    "from keras.datasets import mnist\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# create a grid of 3x3 images\n",
    "for i in range(0, 9):\n",
    "    pyplot.subplot(330 + 1 + i)\n",
    "    pyplot.imshow(X_train[i], cmap=pyplot.get_cmap('gray'))\n",
    "\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize images across the dataset, mean=0, stdev=1\n",
    "from keras.datasets import mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "\n",
    "# convert from int to float\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# define data preparation\n",
    "datagen = ImageDataGenerator(featurewise_center=True, featurewise_std_normalization=True)\n",
    "\n",
    "# fit parameters from data\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# configure batch size and retrieve one batch of images\n",
    "for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9):\n",
    "    # create a grid of 3x3 images\n",
    "    for i in range(0, 9):\n",
    "        pyplot.subplot(330 + 1 + i)\n",
    "        pyplot.imshow(X_batch[i].reshape(28, 28), cmap=pyplot.get_cmap('gray'))\n",
    "\n",
    "    # show the plot\n",
    "    pyplot.show()\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random flips\n",
    "from keras.datasets import mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "\n",
    "# convert from int to float\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# define data preparation\n",
    "datagen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True)\n",
    "\n",
    "# fit parameters from data\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# configure batch size and retrieve one batch of images\n",
    "for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9):\n",
    "    # create a grid of 3x3 images\n",
    "    for i in range(0, 9):\n",
    "        pyplot.subplot(330 + 1 + i)\n",
    "        pyplot.imshow(X_batch[i].reshape(28, 28), cmap=pyplot.get_cmap('gray'))\n",
    "    # show the plot\n",
    "    pyplot.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random rotations\n",
    "from keras.datasets import mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "\n",
    "# convert from int to float\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# define data preparation\n",
    "datagen = ImageDataGenerator(rotation_range=90)\n",
    "\n",
    "# fit parameters from data\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# configure batch size and retrieve one batch of images\n",
    "for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9):\n",
    "    # create a grid of 3x3 images\n",
    "    for i in range(0, 9):\n",
    "        pyplot.subplot(330 + 1 + i)\n",
    "        pyplot.imshow(X_batch[i].reshape(28, 28), cmap=pyplot.get_cmap('gray'))\n",
    "    \n",
    "    # show the plot\n",
    "    pyplot.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save augmented images to file\n",
    "from keras.datasets import mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot\n",
    "import os\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "\n",
    "# convert from int to float\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# define data preparation\n",
    "datagen = ImageDataGenerator()\n",
    "\n",
    "# fit parameters from data\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# configure batch size and retrieve one batch of images\n",
    "os.makedirs('images')\n",
    "\n",
    "for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9, save_to_dir='images', save_prefix='aug', save_format='png'):\n",
    "    # create a grid of 3x3 images\n",
    "    for i in range(0, 9):\n",
    "        pyplot.subplot(330 + 1 + i)\n",
    "        pyplot.imshow(X_batch[i].reshape(28, 28), cmap=pyplot.get_cmap('gray'))\n",
    "    \n",
    "    # show the plot\n",
    "    pyplot.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random shifts\n",
    "from keras.datasets import mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "\n",
    "# convert from int to float\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# define data preparation\n",
    "shift = 0.2\n",
    "datagen = ImageDataGenerator(width_shift_range=shift, height_shift_range=shift)\n",
    "\n",
    "# fit parameters from data\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# configure batch size and retrieve one batch of images\n",
    "for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9):\n",
    "    # create a grid of 3x3 images\n",
    "    for i in range(0, 9):\n",
    "        pyplot.subplot(330 + 1 + i)\n",
    "        pyplot.imshow(X_batch[i].reshape(28, 28), cmap=pyplot.get_cmap('gray'))\n",
    "    \n",
    "    # show the plot\n",
    "    pyplot.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZCA whitening\n",
    "from keras.datasets import mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# reshape to be [samples][pixels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "\n",
    "# convert from int to float\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# define data preparation\n",
    "datagen = ImageDataGenerator(zca_whitening=True)\n",
    "\n",
    "# fit parameters from data\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# configure batch size and retrieve one batch of images\n",
    "for X_batch, y_batch in datagen.flow(X_train, y_train, batch_size=9):\n",
    "    # create a grid of 3x3 images\n",
    "    for i in range(0, 9):\n",
    "        pyplot.subplot(330 + 1 + i)\n",
    "        pyplot.imshow(X_batch[i].reshape(28, 28), cmap=pyplot.get_cmap('gray'))\n",
    "\n",
    "    # show the plot\n",
    "    pyplot.show()\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple CNN model for the CIFAR-10 dataset\n",
    "import numpy\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "# random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# normalize inputs from 0-255 to 0.0-1.0\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(32, 3, 3, input_shape=(3, 32, 32), border_mode='same', activation='relu', W_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Convolution2D(32, 3, 3, activation='relu', border_mode='same', W_constraint=maxnorm(3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu', W_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# compile the model\n",
    "epochs = 25\n",
    "lrate = 0.01\n",
    "decay = lrate/epochs\n",
    "sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=epochs, batch_size=32, verbose=2)\n",
    "\n",
    "# final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# large CNN model for the CIFAR-10 Dataset\n",
    "import numpy\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "# random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# normalize inputs from 0-255 to 0.0-1.0\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(32, 3, 3, input_shape=(3, 32, 32), activation='relu', border_mode='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Convolution2D(32, 3, 3, activation='relu', border_mode='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(64, 3, 3, activation='relu', border_mode='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Convolution2D(64, 3, 3, activation='relu', border_mode='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Convolution2D(128, 3, 3, activation='relu', border_mode='same'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Convolution2D(128, 3, 3, activation='relu', border_mode='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1024, activation='relu', W_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu', W_constraint=maxnorm(3)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# compile the model\n",
    "epochs = 25\n",
    "lrate = 0.01\n",
    "decay = lrate/epochs\n",
    "sgd = SGD(lr=lrate, momentum=0.9, decay=decay, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=epochs, batch_size=64)\n",
    "\n",
    "# final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the ad hoc CIFAR10 instances\n",
    "from keras.datasets import cifar10\n",
    "from matplotlib import pyplot\n",
    "from scipy.misc import toimage\n",
    "\n",
    "# load data\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# create a grid of 3x3 images\n",
    "for i in range(0, 9):\n",
    "    pyplot.subplot(330 + 1 + i)\n",
    "    pyplot.imshow(toimage(X_train[i]))\n",
    "\n",
    "    # show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN for the IMDB problem\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=top_words)\n",
    "\n",
    "# pad dataset to a maximum review length in words\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Convolution1D(nb_filter=32, filter_length=3, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=2, batch_size=128, verbose=1)\n",
    "\n",
    "# final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP for the IMDB problem\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=top_words)\n",
    "max_words = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_words)\n",
    "\n",
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, 32, input_length=max_words))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "# fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), nb_epoch=2, batch_size=128, verbose=1)\n",
    "\n",
    "# final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and plot the IMDB dataset\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# load the dataset\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data()\n",
    "X = numpy.concatenate((X_train, X_test), axis=0)\n",
    "y = numpy.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "# summarize size\n",
    "print(\"Training data: \")\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# summarize number of classes\n",
    "print(\"Classes: \")\n",
    "print(numpy.unique(y))\n",
    "\n",
    "# summarize number of words\n",
    "print(\"Number of words: \")\n",
    "print(len(numpy.unique(numpy.hstack(X))))\n",
    "\n",
    "# summarize review length\n",
    "print(\"Review length: \")\n",
    "result = map(len, X)\n",
    "print(\"Mean %.2f words (%f)\" % (numpy.mean(result), numpy.std(result)))\n",
    "\n",
    "# plot review length as a boxplot and histogram\n",
    "pyplot.subplot(121)\n",
    "pyplot.boxplot(result)\n",
    "pyplot.subplot(122)\n",
    "pyplot.hist(result)\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## International Airline Passengers dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP to predict international airline passengers (t+1, given t)\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# load the dataset\n",
    "dataframe = pandas.read_csv('international-airline-passengers.csv', usecols=[1], engine='python', skipfooter=3)\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "\n",
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "print(len(train), len(test))\n",
    "\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return numpy.array(dataX), numpy.array(dataY)\n",
    "\n",
    "# reshape into X=t and Y=t+1\n",
    "look_back = 1\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "# create and fit MLP model\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=look_back, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, epochs=200, batch_size=2, verbose=2)\n",
    "\n",
    "# estimate model performance\n",
    "trainScore = model.evaluate(trainX, trainY, verbose=0)\n",
    "print('Train Score: %.2f MSE (%.2f RMSE)' % (trainScore, math.sqrt(trainScore)))\n",
    "testScore = model.evaluate(testX, testY, verbose=0)\n",
    "print('Test Score: %.2f MSE (%.2f RMSE)' % (testScore, math.sqrt(testScore)))\n",
    "\n",
    "# generate predictions for training\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "\n",
    "# plot baseline and predictions\n",
    "plt.plot(dataset)\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP to predict international airline passengers (t+1, given t, t-1, t-2)\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return numpy.array(dataX), numpy.array(dataY)\n",
    "\n",
    "# random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# load the dataset\n",
    "dataframe = pandas.read_csv('international-airline-passengers.csv', usecols=[1], engine='python', skipfooter=3)\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "\n",
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "print(len(train), len(test))\n",
    "\n",
    "# reshape dataset\n",
    "look_back = 10\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "# create and fit MLP model\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=look_back, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, nb_epoch=200, batch_size=2, verbose=2)\n",
    "\n",
    "# estimate model performance\n",
    "trainScore = model.evaluate(trainX, trainY, verbose=0)\n",
    "print('Train Score: %.2f MSE (%.2f RMSE)' % (trainScore, math.sqrt(trainScore)))\n",
    "testScore = model.evaluate(testX, testY, verbose=0)\n",
    "print('Test Score: %.2f MSE (%.2f RMSE)' % (testScore, math.sqrt(testScore)))\n",
    "\n",
    "# generate predictions for training\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "\n",
    "# plot baseline and predictions\n",
    "plt.plot(dataset)\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory (LSTMs) Units <a name=\"lstms\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM for international airline passengers problem with regression framing\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return numpy.array(dataX), numpy.array(dataY)\n",
    "\n",
    "# random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# load the dataset\n",
    "dataframe = pandas.read_csv('international-airline-passengers.csv', usecols=[1], engine='python', skipfooter=3)\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "\n",
    "# reshape into X=t and Y=t+1\n",
    "look_back = 1\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_dim=look_back))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, nb_epoch=100, batch_size=1, verbose=2)\n",
    "\n",
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacked LSTM for international airline passengers problem with memory\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return numpy.array(dataX), numpy.array(dataY)\n",
    "\n",
    "# random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# load the dataset\n",
    "dataframe = pandas.read_csv('international-airline-passengers.csv', usecols=[1], engine='python', skipfooter=3)\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "# split data into train and test sets\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "\n",
    "# reshape into X=t and Y=t+1\n",
    "look_back = 3\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "testX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "\n",
    "# create and fit the LSTM network\n",
    "batch_size = 1\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True, return_sequences=True))\n",
    "model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "for i in range(100):\n",
    "    model.fit(trainX, trainY, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)\n",
    "    model.reset_states()\n",
    "\n",
    "# make predictions\n",
    "trainPredict = model.predict(trainX, batch_size=batch_size)\n",
    "model.reset_states()\n",
    "testPredict = model.predict(testX, batch_size=batch_size)\n",
    "\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM for international airline passengers problem with memory\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return numpy.array(dataX), numpy.array(dataY)\n",
    "\n",
    "# random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# load the dataset\n",
    "dataframe = pandas.read_csv('international-airline-passengers.csv', usecols=[1], engine='python', skipfooter=3)\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "\n",
    "# reshape into X=t and Y=t+1\n",
    "look_back = 3\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "testX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "\n",
    "# create and fit the LSTM network\n",
    "batch_size = 1\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "for i in range(100):\n",
    "    model.fit(trainX, trainY, nb_epoch=1, batch_size=batch_size, verbose=2, shuffle=False)\n",
    "    model.reset_states()\n",
    "\n",
    "# make predictions\n",
    "trainPredict = model.predict(trainX, batch_size=batch_size)\n",
    "model.reset_states()\n",
    "testPredict = model.predict(testX, batch_size=batch_size)\n",
    "\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM for international airline passengers problem with time step regression framing\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return numpy.array(dataX), numpy.array(dataY)\n",
    "\n",
    "# random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# load the dataset\n",
    "dataframe = pandas.read_csv('international-airline-passengers.csv', usecols=[1], engine='python', skipfooter=3)\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "# split data into train and test sets\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "\n",
    "# reshape into X=t and Y=t+1\n",
    "look_back = 3\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "testX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "\n",
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_dim=1))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, nb_epoch=100, batch_size=1, verbose=2)\n",
    "\n",
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM for international airline passengers problem with window regression framing\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1):\n",
    "        a = dataset[i:(i+look_back), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])\n",
    "    return numpy.array(dataX), numpy.array(dataY)\n",
    "\n",
    "# random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# load the dataset\n",
    "dataframe = pandas.read_csv('international-airline-passengers.csv', usecols=[1], engine='python', skipfooter=3)\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "# split data into train and test sets\n",
    "train_size = int(len(dataset) * 0.67)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "\n",
    "# reshape into X=t and Y=t+1\n",
    "look_back = 3\n",
    "trainX, trainY = create_dataset(train, look_back)\n",
    "testX, testY = create_dataset(test, look_back)\n",
    "\n",
    "# reshape input to be [samples, time steps, features]\n",
    "trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "\n",
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_dim=look_back))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(trainX, trainY, nb_epoch=100, batch_size=1, verbose=2)\n",
    "\n",
    "# make predictions\n",
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)\n",
    "\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining LSTMs with CNNs <a name=\"lstm_plus_cnn\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM and CNN for sequence classification in the IMDB dataset\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Convolution1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=top_words)\n",
    "\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Convolution1D(nb_filter=32, filter_length=3, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "\n",
    "# final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM with dropout for sequence classification in the IMDB dataset\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=top_words)\n",
    "\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length, dropout=0.2))\n",
    "model.add(LSTM(100, dropout_W=0.2, dropout_U=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "\n",
    "# final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM with dropout for sequence classification in the IMDB dataset\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=top_words)\n",
    "\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length, dropout=0.2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "\n",
    "# final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM for sequence classification in the IMDB dataset\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(nb_words=top_words)\n",
    "\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "\n",
    "# final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive LSTM to learn one-char to one-char mapping with all data in each batch\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 1\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "    seq_in = alphabet[i:i + seq_length]\n",
    "    seq_out = alphabet[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "    print seq_in, '->', seq_out\n",
    "\n",
    "# convert list of lists to array and pad sequences if needed\n",
    "X = pad_sequences(dataX, maxlen=seq_length, dtype='float32')\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (X.shape[0], seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(len(alphabet))\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "# create and fit the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(16, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, nb_epoch=5000, batch_size=len(dataX), verbose=2, shuffle=False)\n",
    "\n",
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "# demonstrate some model predictions\n",
    "for pattern in dataX:\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(len(alphabet))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    print seq_in, \"->\", result\n",
    "\n",
    "# demonstrate predicting random patterns\n",
    "print \"Test a Random Pattern:\"\n",
    "for i in range(0,20):\n",
    "    pattern_index = numpy.random.randint(len(dataX))\n",
    "    pattern = dataX[pattern_index]\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(len(alphabet))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    print seq_in, \"->\", result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive LSTM to learn three-char window to one-char mapping\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 3\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "    seq_in = alphabet[i:i + seq_length]\n",
    "    seq_out = alphabet[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "    print seq_in, '->', seq_out\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (len(dataX), 1, seq_length))\n",
    "\n",
    "# normalize\n",
    "X = X / float(len(alphabet))\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "# create and fit the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, nb_epoch=500, batch_size=1, verbose=2)\n",
    "\n",
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "# demonstrate some model predictions\n",
    "for pattern in dataX:\n",
    "    x = numpy.reshape(pattern, (1, 1, len(pattern)))\n",
    "    x = x / float(len(alphabet))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    print seq_in, \"->\", result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive LSTM to learn three-char time steps to one-char mapping\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 3\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "    seq_in = alphabet[i:i + seq_length]\n",
    "    seq_out = alphabet[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "    print seq_in, '->', seq_out\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (len(dataX), seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(len(alphabet))\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "# create and fit the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=500, batch_size=1, verbose=2)\n",
    "\n",
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "# demonstrate some model predictions\n",
    "for pattern in dataX:\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(len(alphabet))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    print seq_in, \"->\", result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive LSTM to learn one-char to one-char mapping\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 1\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "    seq_in = alphabet[i:i + seq_length]\n",
    "    seq_out = alphabet[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "    print seq_in, '->', seq_out\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (len(dataX), seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(len(alphabet))\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "# create and fit the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=500, batch_size=1, verbose=2)\n",
    "\n",
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "# demonstrate some model predictions\n",
    "for pattern in dataX:\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(len(alphabet))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    print seq_in, \"->\", result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateful LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stateful LSTM to learn one-char to one-char mapping\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 1\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "    seq_in = alphabet[i:i + seq_length]\n",
    "    seq_out = alphabet[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "    print seq_in, '->', seq_out\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (len(dataX), seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(len(alphabet))\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "# create and fit the model\n",
    "batch_size = 1\n",
    "model = Sequential()\n",
    "model.add(LSTM(16, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "for i in range(300):\n",
    "    model.fit(X, y, nb_epoch=1, batch_size=batch_size, verbose=2, shuffle=False)\n",
    "    model.reset_states()\n",
    "\n",
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, batch_size=batch_size, verbose=0)\n",
    "model.reset_states()\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "# demonstrate some model predictions\n",
    "seed = [char_to_int[alphabet[0]]]\n",
    "for i in range(0, len(alphabet)-1):\n",
    "    x = numpy.reshape(seed, (1, len(seed), 1))\n",
    "    x = x / float(len(alphabet))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    print int_to_char[seed[0]], \"->\", int_to_char[index]\n",
    "    seed = [index]\n",
    "\n",
    "model.reset_states()\n",
    "\n",
    "# demonstrate a random starting point\n",
    "letter = \"K\"\n",
    "seed = [char_to_int[letter]]\n",
    "print \"New start: \", letter\n",
    "\n",
    "for i in range(0, 5):\n",
    "    x = numpy.reshape(seed, (1, len(seed), 1))\n",
    "    x = x / float(len(alphabet))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    print int_to_char[seed[0]], \"->\", int_to_char[index]\n",
    "    seed = [index]\n",
    "\n",
    "model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM with variable length input sequences to one character output\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "num_inputs = 1000\n",
    "max_len = 5\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(num_inputs):\n",
    "    start = numpy.random.randint(len(alphabet)-2)\n",
    "    end = numpy.random.randint(start, min(start+max_len,len(alphabet)-1))\n",
    "    sequence_in = alphabet[start:end+1]\n",
    "    sequence_out = alphabet[end + 1]\n",
    "    dataX.append([char_to_int[char] for char in sequence_in])\n",
    "    dataY.append(char_to_int[sequence_out])\n",
    "    print sequence_in, '->', sequence_out\n",
    "\n",
    "# convert list of lists to array and pad sequences if needed\n",
    "X = pad_sequences(dataX, maxlen=max_len, dtype='float32')\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(X, (X.shape[0], max_len, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(len(alphabet))\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "# create and fit the model\n",
    "batch_size = 1\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], 1)))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=500, batch_size=batch_size, verbose=2)\n",
    "\n",
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "# demonstrate some model predictions\n",
    "for i in range(20):\n",
    "    pattern_index = numpy.random.randint(len(dataX))\n",
    "    pattern = dataX[pattern_index]\n",
    "    x = pad_sequences([pattern], maxlen=max_len, dtype='float32')\n",
    "    x = numpy.reshape(x, (1, max_len, 1))\n",
    "    x = x / float(len(alphabet))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    print seq_in, \"->\", result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating text with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load LSTM network and generate text\n",
    "import sys\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "# create mapping of unique chars to integers, and a reverse mapping\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print \"Total Characters: \", n_chars\n",
    "print \"Total Vocab: \", n_vocab\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "print \"Total Patterns: \", n_patterns\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# normalize data\n",
    "X = X / float(n_vocab)\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "# load the network weights\n",
    "filename = \"weights-improvement-19-1.9435.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print \"Seed:\"\n",
    "print \"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\"\n",
    "\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "\n",
    "print \"\\nDone.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# larger LSTM network to generate text for Alice in Wonderland\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print \"Total Characters: \", n_chars\n",
    "print \"Total Vocab: \", n_vocab\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "print \"Total Patterns: \", n_patterns\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Larger LSTM network and generate text\n",
    "import sys\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()\n",
    "# create mapping of unique chars to integers, and a reverse mapping\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print \"Total Characters: \", n_chars\n",
    "print \"Total Vocab: \", n_vocab\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "print \"Total Patterns: \", n_patterns\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "# load the network weights\n",
    "filename = \"weights-improvement-47-1.2219-bigger.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print \"Seed:\"\n",
    "print \"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\"\n",
    "\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "\n",
    "print \"\\nDone.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small LSTM network to generate text for Alice in Wonderland\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print \"Total Characters: \", n_chars\n",
    "print \"Total Vocab: \", n_vocab\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "print \"Total Patterns: \", n_patterns\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# fit the model\n",
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
