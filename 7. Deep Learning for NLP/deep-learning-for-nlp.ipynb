{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning for Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Infrastructure <a name=\"software_infrastructure\"></a>\n",
    "\n",
    "We will use the following libraries and tools:\n",
    "\n",
    "- Python 3\n",
    "- **Matplotlib**, a Python library for plotting.\n",
    "- **Pandas**, a Python library which provides a dataframe data structure, and operations for manipulating numerical tables and time series.\n",
    "- **NumPy** Python library which provides support for large, multi-dimensional arrays and matrices, as well as a large collection of high-level mathematical functions to operate on these arrays. \n",
    "- Python **SciPy** library for scientific computing -- modules for optimization, linear algebra, integration, interpolation, special functions, FFT, signal and image processing, ODE solvers and other tasks common in science and engineering.\n",
    "- Python **scikit-learn**, a machine learning library\n",
    "- **Theano**, a numerical platform for developing deep learning models\n",
    "- **TensorFlow**, another numerical platform for developing deep learning models\n",
    "- **Keras**, a concise Python API for **Theano** and **TensorFlow**\n",
    "- **NLTK**, a Python library for Natural Language Processing (NLP)\n",
    "- **GenSim**, a a Python library for vector space information retrieval modeling and topic modeling\n",
    "- **pydot** and **graphviz** libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Topics\n",
    "\n",
    "- Data cleaning and normalization\n",
    "- Bag-of-words model for text\n",
    "- Distributed representations of text using word embedding models\n",
    "- Text classification\n",
    "- Neural language models for text generation\n",
    "- Neural sentiment analysis (a classification problem)\n",
    "- Neural machine translation (translating text from one language to another)\n",
    "- Generating captions for images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benifits of DL to NLP\n",
    "\n",
    "- Scalability\n",
    "- Hierarchical Feature Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commonly Used Optimization Algorithms\n",
    "\n",
    "- Stochastic Gradient Descent (SGD)\n",
    "- Adam (requires tuning of learning rate)\n",
    "- RMSprop (requires tuning of learning rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction of a Simple Multilayer Perceptron (MLP) for Binary Classification Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construction of a simple MLP \n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "\n",
    "# define input layer\n",
    "visible = Input(shape=(10,))\n",
    "\n",
    "# define hidden layers with relu activation function\n",
    "hidden1 = Dense(10, activation='relu')(visible)\n",
    "hidden2 = Dense(20, activation='relu')(hidden1)\n",
    "hidden3 = Dense(10, activation='relu')(hidden2)\n",
    "\n",
    "# define output layer with sigmoid activation function\n",
    "output = Dense(1, activation='sigmoid')(hidden3)\n",
    "\n",
    "# define MLP model\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "\n",
    "# summarize layers\n",
    "model.summary()\n",
    "\n",
    "# plot graph of the MLP model\n",
    "# plot_model(model, to_file='mlp_model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the MLP model above,\n",
    "\n",
    "   - How many neurons are there in the input layer?\n",
    "   - How many neurons are there in the output layer?\n",
    "   - How many hiddenlayers are there?\n",
    "   - What activation functions are used in each hidden layer?\n",
    "   - What activation function is used in the output layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction of a Simple Convolutional Neural Network (CNN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construction of a simple CNN\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "# input is black-and-white 64 x 64 images\n",
    "\n",
    "# define input layer\n",
    "visible = Input(shape=(64,64,1))\n",
    "\n",
    "# define first convolutional layer\n",
    "conv1 = Conv2D(32, kernel_size=4, activation='relu')(visible)\n",
    "\n",
    "# define first pooling layer\n",
    "pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "# define second convolutional layer with relu activation function\n",
    "conv2 = Conv2D(16, kernel_size=4, activation='relu')(pool1)\n",
    "\n",
    "# define second pooling layer\n",
    "pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "# define hidden layer with relu activation function\n",
    "hidden1 = Dense(10, activation='relu')(pool2)\n",
    "\n",
    "# define output layer with sigmoid activation function\n",
    "output = Dense(1, activation='sigmoid')(hidden1)\n",
    "\n",
    "# define CNN model\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "\n",
    "# summarize layers\n",
    "model.summary()\n",
    "\n",
    "# plot graph of the CNN model\n",
    "# plot_model(model, to_file='cnn_model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction of a Simple Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "# define input layer\n",
    "visible = Input(shape=(100,1))\n",
    "\n",
    "# define hidden layers\n",
    "hidden1 = LSTM(10)(visible)\n",
    "hidden2 = Dense(10, activation='relu')(hidden1)\n",
    "\n",
    "# define output layer\n",
    "output = Dense(1, activation='sigmoid')(hidden2)\n",
    "\n",
    "# define RNN model\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "\n",
    "# summarize layers\n",
    "model.summary()\n",
    "\n",
    "# plot graph of the RNN model\n",
    "# plot_model(model, to_file='rnn_model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textual Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first install NLTK (one time task)\n",
    "sudo pip install -U nltk\n",
    "\n",
    "import nltk\n",
    "\n",
    "# data download (one time task)\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
      "and happy disposition, seemed to unite some of the best blessings\n",
      "of existence; and had lived nearly twenty-one years in the world\n",
      "with very little to distress or vex her.\n",
      "['[Emma', 'by', 'Jane', 'Austen', '1816]', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse,', 'handsome,', 'clever,', 'and', 'rich,', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition,', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence;', 'and', 'had', 'lived', 'nearly', 'twenty-one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her.', 'She', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daughters', 'of', 'a', 'most', 'affectionate,', 'indulgent', 'father;', 'and', 'had,', 'in', 'consequence', 'of', 'her', \"sister's\", 'marriage,', 'been', 'mistress', 'of', 'his', 'house', 'from', 'a', 'very', 'early', 'period.', 'Her', 'mother', 'had', 'died', 'too', 'long', 'ago', 'for', 'her', 'to', 'have', 'more', 'than', 'an', 'indistinct', 'remembrance', 'of', 'her', 'caresses;']\n",
      "['', 'Emma', 'by', 'Jane', 'Austen', '1816', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', 'handsome', 'clever', 'and', 'rich', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', 'and', 'had', 'lived', 'nearly', 'twenty', 'one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', 'She', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daughters', 'of', 'a', 'most', 'affectionate', 'indulgent', 'father', 'and', 'had', 'in', 'consequence', 'of', 'her', 'sister', 's', 'marriage', 'been', 'mistress', 'of', 'his', 'house', 'from', 'a', 'very', 'early', 'period', 'Her', 'mother', 'had', 'died', 'too', 'long', 'ago', 'for', 'her', 'to', 'have', 'more', 'than', 'an', 'indistinct', 'remembrance']\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "['[emma', 'by', 'jane', 'austen', '1816]', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse,', 'handsome,', 'clever,', 'and', 'rich,', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition,', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence;', 'and', 'had', 'lived', 'nearly', 'twenty-one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her.', 'she', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daughters', 'of', 'a', 'most', 'affectionate,', 'indulgent', 'father;', 'and', 'had,', 'in', 'consequence', 'of', 'her', \"sister's\", 'marriage,', 'been', 'mistress', 'of', 'his', 'house', 'from', 'a', 'very', 'early', 'period.', 'her', 'mother', 'had', 'died', 'too', 'long', 'ago', 'for', 'her', 'to', 'have', 'more', 'than', 'an', 'indistinct', 'remembrance', 'of', 'her', 'caresses;']\n",
      "['emma', 'by', 'jane', 'austen', '1816', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse', 'handsome', 'clever', 'and', 'rich', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', 'and', 'had', 'lived', 'nearly', 'twentyone', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', 'she', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daughters', 'of', 'a', 'most', 'affectionate', 'indulgent', 'father', 'and', 'had', 'in', 'consequence', 'of', 'her', 'sisters', 'marriage', 'been', 'mistress', 'of', 'his', 'house', 'from', 'a', 'very', 'early', 'period', 'her', 'mother', 'had', 'died', 'too', 'long', 'ago', 'for', 'her', 'to', 'have', 'more', 'than', 'an', 'indistinct', 'remembrance', 'of', 'her', 'caresses']\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import re\n",
    "\n",
    "# load text file\n",
    "filename = 'austen-emma.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "# split text into sentences\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences[0])\n",
    "\n",
    "# split text into words by white space as delimiter\n",
    "words = text.split()\n",
    "\n",
    "# print first one hundred words\n",
    "print(words[:100])\n",
    "\n",
    "# or, you can also split text using words as delimiters\n",
    "# define what a word is using a regular expression\n",
    "# strings of alphanumeric characters (a-z, A-Z, 0-9 and _ (underscore)\n",
    "words = re.split(r'\\W+', text)\n",
    "\n",
    "# print first one hundred words\n",
    "print(words[:100])\n",
    "\n",
    "# which are Python punctuation characters?\n",
    "print(string.punctuation)\n",
    "\n",
    "# as before, split text into words using white space\n",
    "words = text.split()\n",
    "\n",
    "# normalization - convert to lower case\n",
    "words = [word.lower() for word in words]\n",
    "print(words[:100])\n",
    "\n",
    "# replace punctuation characters with nothing\n",
    "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "# remove punctuation from each word\n",
    "stripped = [re_punc.sub('', w) for w in words]\n",
    "\n",
    "# functuation-free words\n",
    "print(stripped[:100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# NLTK stopword list\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty-one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', '.', 'She', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daughters', 'of', 'a', 'most', 'affectionate', ',', 'indulgent', 'father', ';', 'and', 'had', ',', 'in', 'consequence', 'of', 'her', 'sister', \"'s\", 'marriage', ',', 'been', 'mistress', 'of', 'his', 'house', 'from', 'a', 'very', 'early', 'period', '.', 'Her', 'mother', 'had', 'died']\n",
      "['emma', 'by', 'jane', 'austen', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse', 'handsome', 'clever', 'and', 'rich', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', 'and', 'had', 'lived', 'nearly', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', 'she', 'was', 'the', 'youngest', 'of', 'the', 'two', 'daughters', 'of', 'a', 'most', 'affectionate', 'indulgent', 'father', 'and', 'had', 'in', 'consequence', 'of', 'her', 'sister', 'marriage', 'been', 'mistress', 'of', 'his', 'house', 'from', 'a', 'very', 'early', 'period', 'her', 'mother', 'had', 'died', 'too', 'long', 'ago', 'for', 'her', 'to', 'have', 'more', 'than', 'an', 'indistinct', 'remembrance', 'of', 'her', 'caresses', 'and', 'her']\n",
      "['emma', 'jane', 'austen', 'volume', 'chapter', 'emma', 'woodhouse', 'handsome', 'clever', 'rich', 'comfortable', 'home', 'happy', 'disposition', 'seemed', 'unite', 'best', 'blessings', 'existence', 'lived', 'nearly', 'years', 'world', 'little', 'distress', 'vex', 'youngest', 'two', 'daughters', 'affectionate', 'indulgent', 'father', 'consequence', 'sister', 'marriage', 'mistress', 'house', 'early', 'period', 'mother', 'died', 'long', 'ago', 'indistinct', 'remembrance', 'caresses', 'place', 'supplied', 'excellent', 'woman', 'governess', 'fallen', 'little', 'short', 'mother', 'affection', 'sixteen', 'years', 'miss', 'taylor', 'woodhouse', 'family', 'less', 'governess', 'friend', 'fond', 'daughters', 'particularly', 'emma', 'intimacy', 'sisters', 'even', 'miss', 'taylor', 'ceased', 'hold', 'nominal', 'office', 'governess', 'mildness', 'temper', 'hardly', 'allowed', 'impose', 'restraint', 'shadow', 'authority', 'long', 'passed', 'away', 'living', 'together', 'friend', 'friend', 'mutually', 'attached', 'emma', 'liked', 'highly', 'esteeming']\n"
     ]
    }
   ],
   "source": [
    "# NLTK tokenizer and stemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "filename = 'austen-emma.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "# split into words using NLTK tokenizer\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens[:100])\n",
    "\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "\n",
    "# regex for punctutaion character filtering\n",
    "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "# remove punctuation from each word\n",
    "stripped = [re_punc.sub('', w) for w in tokens]\n",
    "\n",
    "# remove tokens that are not alphabetic\n",
    "words = [word for word in tokens if word.isalpha()]\n",
    "print(words[:100])\n",
    "\n",
    "# finally, remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "words = [w for w in words if not w in stop_words]\n",
    "print(words[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'emma', 'by', 'jane', 'austen', '1816', ']', 'volum', 'i', 'chapter', 'i', 'emma', 'woodhous', ',', 'handsom', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfort', 'home', 'and', 'happi', 'disposit', ',', 'seem', 'to', 'unit', 'some', 'of', 'the', 'best', 'bless', 'of', 'exist', ';', 'and', 'had', 'live', 'nearli', 'twenty-on', 'year', 'in', 'the', 'world', 'with', 'veri', 'littl', 'to', 'distress', 'or', 'vex', 'her', '.', 'she', 'wa', 'the', 'youngest', 'of', 'the', 'two', 'daughter', 'of', 'a', 'most', 'affection', ',', 'indulg', 'father', ';', 'and', 'had', ',', 'in', 'consequ', 'of', 'her', 'sister', \"'s\", 'marriag', ',', 'been', 'mistress', 'of', 'hi', 'hous', 'from', 'a', 'veri', 'earli', 'period', '.', 'her', 'mother', 'had', 'die']\n"
     ]
    }
   ],
   "source": [
    "# word stemming using NLTK\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "filename = 'austen-emma.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "# Porter stemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# stem words\n",
    "stemmed = [porter.stem(word) for word in tokens]\n",
    "print(stemmed[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words Model (BoW)\n",
    "\n",
    "- What matters is whether a words occurs in a document, and if so, how many times does it occur.\n",
    "\n",
    "- First determine domain vocabulary.\n",
    "\n",
    "- Then each document can be represented as a fixed size vector, and the length of the vector = domain vocabulary size.\n",
    "\n",
    "- Word frequency information can be easily incorporated, and the vector now becomes a weighted vector.\n",
    "\n",
    "- Next, we illustrate how to construct word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word': 12, 'frequency': 5, 'information': 7, 'can': 3, 'be': 1, 'easily': 4, 'incorporated': 6, 'and': 0, 'the': 9, 'vector': 10, 'now': 8, 'becomes': 2, 'weighted': 11}\n",
      "(1, 13)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[1 1 1 1 1 1 1 1 1 1 2 1 1]]\n",
      "(1, 13)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[0 0 0 0 0 0 0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# list of text documents\n",
    "text = [\"Word frequency information can be easily incorporated, and the vector now becomes a weighted vector.\"]\n",
    "\n",
    "# using CountVectorizer, \n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# tokenize and build vector from sample text\n",
    "vectorizer.fit(text)\n",
    "\n",
    "# summarize vocabulary\n",
    "print(vectorizer.vocabulary_)\n",
    "\n",
    "# encode document, but the result will be a sparse vector\n",
    "vector = vectorizer.transform(text)\n",
    "\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "\n",
    "# transfer sparse vector back into NumPy arrays\n",
    "print(vector.toarray())\n",
    "\n",
    "# we can encode another document with the same vocabulary\n",
    "text = [\"Greenville information.\"]\n",
    "vector = vectorizer.transform(text)\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "# transfer sparse vector back into NumPy arrays\n",
    "print(vector.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Weighting Using Term Frequency - Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word': 25, 'frequency': 8, 'information': 11, 'can': 4, 'be': 2, 'easily': 6, 'incorporated': 10, 'and': 0, 'the': 17, 'vector': 19, 'now': 13, 'becomes': 3, 'weighted': 22, 'we': 21, 'encode': 7, 'another': 1, 'document': 5, 'with': 24, 'same': 14, 'vocabulary': 20, 'term': 15, 'weighting': 23, 'using': 18, 'inverse': 12, 'tf': 16, 'idf': 9}\n",
      "[1.69314718 1.69314718 1.69314718 1.69314718 1.28768207 1.28768207\n",
      " 1.69314718 1.69314718 1.28768207 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 1.28768207\n",
      " 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.69314718]\n",
      "(1, 26)\n",
      "[[0.26050857 0.         0.26050857 0.26050857 0.19812348 0.\n",
      "  0.26050857 0.         0.19812348 0.         0.26050857 0.26050857\n",
      "  0.         0.26050857 0.         0.         0.         0.19812348\n",
      "  0.         0.52101713 0.         0.         0.26050857 0.\n",
      "  0.         0.26050857]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# list of text documents\n",
    "text = [\"Word frequency information can be easily incorporated, and the vector now becomes a weighted vector.\",\n",
    "        \"we can encode another document with the same vocabulary.\",\n",
    "        \"Term Weighting Using Term Frequency - Inverse Document Frequency (TF-IDF).\"]\n",
    "\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# tokenize and build vocabulary\n",
    "vectorizer.fit(text)\n",
    "\n",
    "# summarize the TF-IDF \n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)\n",
    "\n",
    "# encode the first document\n",
    "vector = vectorizer.transform([text[0]])\n",
    "\n",
    "# summarize encoded document vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashing\n",
    "\n",
    "- Hash words so that they are converted into integers.\n",
    "\n",
    "- Avoids problems that arise due to large vocabulary.\n",
    "\n",
    "- However, no way to convert integer values back to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20)\n",
      "[[-0.37796447  0.          0.          0.          0.          0.\n",
      "   0.         -0.37796447  0.          0.          0.          0.37796447\n",
      "  -0.37796447  0.          0.          0.          0.37796447 -0.37796447\n",
      "  -0.37796447  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# a text document\n",
    "text = [\"we can encode another document with the same vocabulary.\"]\n",
    "\n",
    "# create the transform, vector size of 20 is arbitrarily chosen\n",
    "vectorizer = HashingVectorizer(n_features=20)\n",
    "\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "\n",
    "# summarize encoded vector of the document\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Text Data with Keras Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['term', 'weighting', 'using', 'term', 'frequency', 'inverse', 'document', 'frequency', 'tf', 'idf']\n"
     ]
    }
   ],
   "source": [
    "# split text into words\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "# define the document\n",
    "text = 'Term Weighting Using Term Frequency - Inverse Document Frequency (TF-IDF).'\n",
    "\n",
    "# tokenize the document using space as the delimeter\n",
    "result = text_to_word_sequence(text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "# sample document\n",
    "text = 'Term Weighting Using Term Frequency - Inverse Document Frequency (TF-IDF).'\n",
    "\n",
    "# estimate the size of the vocabulary\n",
    "words = set(text_to_word_sequence(text))\n",
    "print(words)\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)\n",
    "\n",
    "# integer encode the document\n",
    "result = one_hot(text, round(vocab_size*1.3))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[7, 5, 9, 7, 5, 1, 9, 5, 9, 6]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import hashing_trick\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "# sample document\n",
    "text = 'Term Weighting Using Term Frequency - Inverse Document Frequency (TF-IDF).'\n",
    "\n",
    "# estimate the size of the vocabulary\n",
    "words = set(text_to_word_sequence(text))\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)\n",
    "\n",
    "# integer encode the sample document using md5 hash function\n",
    "result = hashing_trick(text, round(vocab_size*1.3), hash_function='md5')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Tokenizer class for preparing text documents for deep learning\n",
    "\n",
    "- Suitable for large projects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('word', 1), ('frequency', 3), ('information', 1), ('can', 2), ('be', 1), ('easily', 1), ('incorporated', 1), ('and', 1), ('the', 2), ('vector', 2), ('now', 1), ('becomes', 1), ('a', 1), ('weighted', 1), ('we', 1), ('encode', 1), ('another', 1), ('document', 2), ('with', 1), ('same', 1), ('vocabulary', 1), ('term', 2), ('weighting', 1), ('using', 1), ('inverse', 1), ('tf', 1), ('idf', 1)])\n",
      "3\n",
      "{'frequency': 1, 'can': 2, 'the': 3, 'vector': 4, 'document': 5, 'term': 6, 'word': 7, 'information': 8, 'be': 9, 'easily': 10, 'incorporated': 11, 'and': 12, 'now': 13, 'becomes': 14, 'a': 15, 'weighted': 16, 'we': 17, 'encode': 18, 'another': 19, 'with': 20, 'same': 21, 'vocabulary': 22, 'weighting': 23, 'using': 24, 'inverse': 25, 'tf': 26, 'idf': 27}\n",
      "defaultdict(<class 'int'>, {'easily': 1, 'can': 2, 'be': 1, 'and': 1, 'frequency': 2, 'incorporated': 1, 'now': 1, 'a': 1, 'becomes': 1, 'information': 1, 'the': 2, 'word': 1, 'vector': 1, 'weighted': 1, 'we': 1, 'encode': 1, 'same': 1, 'another': 1, 'document': 2, 'vocabulary': 1, 'with': 1, 'inverse': 1, 'term': 1, 'using': 1, 'idf': 1, 'weighting': 1, 'tf': 1})\n",
      "[[0. 1. 1. 1. 2. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 2. 0. 0. 0. 1. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# define sample documents\n",
    "docs = [\"Word frequency information can be easily incorporated, and the vector now becomes a weighted vector.\",\n",
    "        \"we can encode another document with the same vocabulary.\",\n",
    "        \"Term Weighting Using Term Frequency - Inverse Document Frequency (TF-IDF).\"]\n",
    "\n",
    "# create a tokenizer\n",
    "tok = Tokenizer()\n",
    "\n",
    "# fit the tokenizer on the sample documents\n",
    "tok.fit_on_texts(docs)\n",
    "\n",
    "# results of applying tokenizer on sample documents\n",
    "\n",
    "# dictionary of words and their counts\n",
    "print(tok.word_counts)\n",
    "\n",
    "# dictionary of words and how many documents each appears in\n",
    "print(tok.document_count)\n",
    "\n",
    "# dictionary of words and their uniquely assigned integers\n",
    "print(tok.word_index)\n",
    "\n",
    "# integer count of the total number of documents that were used to fit the Tokenizer\n",
    "print(tok.word_docs)\n",
    "\n",
    "# integer encode documents\n",
    "encoded_docs = tok.texts_to_matrix(docs, mode='count')\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words (BoW) Model\n",
    "\n",
    "- Predefined vocabulary.\n",
    "\n",
    "- Measuring the frequency of vocabulary words in document instances and creating documet vectors.\n",
    "\n",
    "    - Binary scoring\n",
    "    \n",
    "    - Frequency/raw counts\n",
    "    \n",
    "    - Relative frequency\n",
    "    \n",
    "    - Word hashing with frequency counts\n",
    "\n",
    "    - TF-IDF\n",
    "\n",
    "- Issues: vocabulary size, sparse representation, context discarded.\n",
    "\n",
    "- Unigram, bigram, and trigram models.\n",
    "\n",
    "- A **bag-of-bigrams** representation is much more powerful than **bag-of-words** representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie Review Dataset (aka Polarity Dataset)\n",
    "\n",
    "- 1000 positive and 1000 negative reviews \n",
    "\n",
    "- All were written before 2002\n",
    "\n",
    "- A cap of 20 reviews per author\n",
    "\n",
    "- 312 authors total per category\n",
    "\n",
    "- Download the dataset (review polarity.tar.gz, 3MB)\n",
    "\n",
    "    - [Download Movie Review Polarity Dataset](http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz)\n",
    "    \n",
    "    \n",
    "- After downloading, unzip, and reviews are in a directory called txt_sentoken\n",
    "\n",
    "- The subdirectories **pos** and **neg** contain postive and negative reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46557\n",
      "[('film', 8860), ('one', 5521), ('movie', 5440), ('like', 3553), ('even', 2555), ('good', 2320), ('time', 2283), ('story', 2118), ('films', 2102), ('would', 2042), ('much', 2024), ('also', 1965), ('characters', 1947), ('get', 1921), ('character', 1906), ('two', 1825), ('first', 1768), ('see', 1730), ('well', 1694), ('way', 1668), ('make', 1590), ('really', 1563), ('little', 1491), ('life', 1472), ('plot', 1451), ('people', 1420), ('movies', 1416), ('could', 1395), ('bad', 1374), ('scene', 1373), ('never', 1364), ('best', 1301), ('new', 1277), ('many', 1268), ('doesnt', 1267), ('man', 1266), ('scenes', 1265), ('dont', 1210), ('know', 1207), ('hes', 1150), ('great', 1141), ('another', 1111), ('love', 1089), ('action', 1078), ('go', 1075), ('us', 1065), ('director', 1056), ('something', 1048), ('end', 1047), ('still', 1038)]\n",
      "14803\n"
     ]
    }
   ],
   "source": [
    "# vocabulary generation for the polarity dataset\n",
    "\n",
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# extract tokens from review document\n",
    "def extract_tokens(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # regular expression for identification of punctuation characters\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation characters from each token\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # remove stop word tokens\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # remove short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# load a movie review into memory\n",
    "def load_review(filename):\n",
    "    # open file in read only mode\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "# extract tokens from review document and add to vocabulary\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    # load doc\n",
    "    doc = load_review(filename)\n",
    "    # extract tokens\n",
    "    tokens = extract_tokens(doc)\n",
    "    # update counts\n",
    "    vocab.update(tokens)\n",
    "\n",
    "# load all reviews in a directory\n",
    "def process_docs(directory, vocab):\n",
    "    # traverse files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            next\n",
    "        # full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # add review to vocabulary\n",
    "        add_doc_to_vocab(path, vocab)\n",
    "\n",
    "# save list to a file\n",
    "def save_list(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "# define vocabulary\n",
    "vocab = Counter()\n",
    "\n",
    "# extract vocabulary from all movie reviews\n",
    "# positive reviews\n",
    "process_docs('txt_sentoken/pos', vocab)\n",
    "# negative reviews\n",
    "process_docs('txt_sentoken/neg', vocab)\n",
    "\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "\n",
    "# print the top words in the vocab\n",
    "print(vocab.most_common(50))\n",
    "\n",
    "# keep tokens with > 5 occurrence\n",
    "min_occurane = 5\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
    "print(len(tokens))\n",
    "\n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, 'vocabulary.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vocabulary and process positive and negative reviews\n",
    "\n",
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# load a document\n",
    "def load_doc(filename):\n",
    "    # open file in read only mode\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# save list to a file\n",
    "def save_list(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "# extract tokens from review document and add to vocabulary\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    # load doc\n",
    "    doc = load_review(filename)\n",
    "    # extract tokens\n",
    "    tokens = extract_tokens(doc)\n",
    "    # update counts\n",
    "    vocab.update(tokens)\n",
    "\n",
    "# load all reviews in a directory\n",
    "def process_docs(directory, vocab):\n",
    "    # traverse files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip files that do not have the right extension\n",
    "        if not filename.endswith(\".txt\"):\n",
    "            next\n",
    "        # full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # add review to vocabulary\n",
    "        add_doc_to_vocab(path, vocab)\n",
    "    \n",
    "    \n",
    "filename = 'vocabulary.txt'\n",
    "vocabulary = load_doc(filename)\n",
    "\n",
    "# extract individual vocabulary items\n",
    "vocab = vocabulary.split()\n",
    "vocab = set(vocab)\n",
    "\n",
    "# process positive reviews using the vocabulary\n",
    "positive_reviews = process_docs('txt_sentoken/pos', vocab)\n",
    "save_list(positive_reviews, 'positive.txt')\n",
    "\n",
    "# process negative reviews using the vocabulary\n",
    "negative_reviews = process_docs('txt_sentoken/neg', vocab)\n",
    "save_list(negative_reviews, 'negative.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be deleted later\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis Using BoW Model\n",
    "\n",
    "- We will use the last 100 positive reviews and the last 100 negative reviews as a test set.\n",
    "\n",
    "- The remaining 1800 reviews will be used as the training dataset.\n",
    "\n",
    "- Reviews named 000 to 899 will be used for training data and reviews named 900 and above for test data.\n",
    "\n",
    "- In the following we reproduce the above code with a minor change for skipping tests data for vocabulary generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary generation for the polarity dataset, skip test data\n",
    "\n",
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# extract tokens from review document\n",
    "def extract_tokens(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # regular expression for identification of punctuation characters\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation characters from each token\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # remove stop word tokens\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # remove short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "# load a movie review into memory\n",
    "def load_review(filename):\n",
    "    # open file in read only mode\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# extract tokens from review document and add to vocabulary\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    # load doc\n",
    "    doc = load_doc(filename)\n",
    "    # extract tokens\n",
    "    tokens = extract_tokens(doc)\n",
    "    # update counts\n",
    "    vocab.update(tokens)\n",
    "\n",
    "# load all reviews in a directory\n",
    "def process_docs(directory, vocab):\n",
    "    # traverse all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip reviews in the test dataset\n",
    "        if filename.startswith('cv9'):\n",
    "            continue\n",
    "        # full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # add review to vocabulary\n",
    "        add_doc_to_vocab(path, vocab)\n",
    "\n",
    "# save list to a file\n",
    "def save_list(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "# define vocabulary\n",
    "vocab = Counter()\n",
    "\n",
    "# extract vocabulary from all movie reviews\n",
    "# positive reviews\n",
    "process_docs('txt_sentoken/pos', vocab)\n",
    "# negative reviews\n",
    "process_docs('txt_sentoken/neg', vocab)\n",
    "\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "\n",
    "# print the top words in the vocab\n",
    "print(vocab.most_common(50))\n",
    "\n",
    "# keep tokens with > 5 occurrences\n",
    "min_occurrence = 5\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurrence]\n",
    "print(len(tokens))\n",
    "\n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, 'vocabulary.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW Representation for Moview Review Dataset\n",
    "\n",
    "- Each document will be represented as a vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process both positive and negative reviews in the training dataset \n",
    "# using the vocabulary developed earlier\n",
    "\n",
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# load a movie review into memory\n",
    "def load_review(filename):\n",
    "    # open file in read only mode\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# extract tokens from review document\n",
    "def extract_tokens(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # regular expression for identification of punctuation characters\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation characters from each token\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # remove stop word tokens\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # remove short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "# load a review and clean it\n",
    "# return only those tokens that are also in the vocabulary\n",
    "def review_to_vocab(filename, vocab):\n",
    "    # load the review\n",
    "    review = load_review(filename)\n",
    "    # clean the review\n",
    "    tokens = extract_tokens(review)\n",
    "    # keep only those tokens that are also in the vocabulary\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# process all reviews in a directory\n",
    "def process_reviews(directory, vocab):\n",
    "    lines = list()\n",
    "    # traverse files in the directory\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load and clean the doc\n",
    "        line = review_to_vocab(path, vocab)\n",
    "        # add to list\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "\n",
    "# load and process a dataset\n",
    "def process_dataset(vocab):\n",
    "    # load documents\n",
    "    # positive reviews\n",
    "    positive = process_reviews('txt_sentoken/pos', vocab)\n",
    "    # positive reviews\n",
    "    negative = process_reviews('txt_sentoken/neg', vocab)\n",
    "    docs = positive + negative\n",
    "    # prepare class labels - 0 for nesgtaive review and 1 for positive review\n",
    "    labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
    "    return docs, labels\n",
    "\n",
    "# load the vocabulary\n",
    "filename = 'vocabulary.txt'\n",
    "vocabulary = load_review(filename)\n",
    "vocab = vocabulary.split()\n",
    "vocab = set(vocab)\n",
    "\n",
    "# load all reviews in training dataset\n",
    "docs, labels = process_dataset(vocab)\n",
    "\n",
    "# summarize results\n",
    "print(len(docs), len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Shortcut to Converting Movie Reviews to BoW Vectors\n",
    "\n",
    "- Keras API provides Tokenizer class to transform documents into encoded vectors.\n",
    "\n",
    "- We will create a Tokenizer and fit it on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform documents into encoded vectors using Tokenizer class\n",
    "\n",
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# load a movie review into memory\n",
    "def load_review(filename):\n",
    "    # open file in read only mode\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# extract tokens from review document\n",
    "def extract_tokens(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # regular expression for identification of punctuation characters\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation characters from each token\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # remove stop word tokens\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # remove short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "# load a review and clean it\n",
    "# return only those tokens that are also in the vocabulary\n",
    "def review_to_vocab(filename, vocab):\n",
    "    # load the review\n",
    "    review = load_review(filename)\n",
    "    # clean the review\n",
    "    tokens = extract_tokens(review)\n",
    "    # keep only those tokens that are also in the vocabulary\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# process all reviews in a directory\n",
    "def process_reviews(directory, vocab, is_train):\n",
    "    lines = list()\n",
    "    # traverse files in the directory\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load and clean the doc\n",
    "        line = review_to_vocab(path, vocab)\n",
    "        # add to list\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "\n",
    "# load and process a dataset\n",
    "def process_dataset(vocab, is_train):\n",
    "    # load documents\n",
    "    # positive reviews\n",
    "    positive = process_reviews('txt_sentoken/pos', vocab, is_train)\n",
    "    # positive reviews\n",
    "    negative = process_reviews('txt_sentoken/neg', vocab, is_train)\n",
    "    docs = positive + negative\n",
    "    # prepare class labels - 0 for nesgtaive review and 1 for positive review\n",
    "    labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
    "    return docs, labels\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# load vocabulary\n",
    "filename = 'vocabulary.txt'\n",
    "vocabulary = load_doc(filename)\n",
    "vocab = set(vocabulary.split())\n",
    "\n",
    "# load all reviews\n",
    "train_docs, ytrain = process_dataset(vocab, True)\n",
    "test_docs, ytest = process_dataset(vocab, False)\n",
    "\n",
    "# create the tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "\n",
    "# encode data\n",
    "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='freq')\n",
    "Xtest = tokenizer.texts_to_matrix(test_docs, mode='freq')\n",
    "\n",
    "print(Xtrain.shape, Xtest.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP for Sentiment Analysis of Movie Review Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# load a movie review into memory\n",
    "def load_review(filename):\n",
    "    # open file in read only mode\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# extract tokens from review document\n",
    "def extract_tokens(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # regular expression for identification of punctuation characters\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation characters from each token\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # remove stop word tokens\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # remove short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "# load a review and clean it\n",
    "# return only those tokens that are also in the vocabulary\n",
    "def review_to_vocab(filename, vocab):\n",
    "    # load the review\n",
    "    review = load_review(filename)\n",
    "    # clean the review\n",
    "    tokens = extract_tokens(review)\n",
    "    # keep only those tokens that are also in the vocabulary\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# process all reviews in a directory\n",
    "def process_reviews(directory, vocab, is_train):\n",
    "    lines = list()\n",
    "    # traverse files in the directory\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load and clean the doc\n",
    "        line = review_to_vocab(path, vocab)\n",
    "        # add to list\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "\n",
    "# load and process a dataset\n",
    "def process_dataset(vocab, is_train):\n",
    "    # load documents\n",
    "    # positive reviews\n",
    "    positive = process_reviews('txt_sentoken/pos', vocab, is_train)\n",
    "    # positive reviews\n",
    "    negative = process_reviews('txt_sentoken/neg', vocab, is_train)\n",
    "    docs = positive + negative\n",
    "    # prepare class labels - 0 for nesgtaive review and 1 for positive review\n",
    "    labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
    "    return docs, labels\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# load vocabulary\n",
    "filename = 'vocabulary.txt'\n",
    "vocabulary = load_doc(filename)\n",
    "vocab = set(vocabulary.split())\n",
    "\n",
    "# load all reviews\n",
    "train_docs, ytrain = process_dataset(vocab, True)\n",
    "test_docs, ytest = process_dataset(vocab, False)\n",
    "\n",
    "# create the tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "\n",
    "# encode data\n",
    "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='freq')\n",
    "Xtest = tokenizer.texts_to_matrix(test_docs, mode='freq')\n",
    "\n",
    "# define MLP model\n",
    "def define_model(n_words):\n",
    "    # define the model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize the model\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "# build/fit the model\n",
    "n_words = Xtest.shape[1]\n",
    "model = define_model(n_words)\n",
    "\n",
    "# fit the model using training data\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
    "\n",
    "# evaluate the model using test data\n",
    "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Word Scoring Methods\n",
    "\n",
    "- **binary** (a word is present or absent)\n",
    "\n",
    "- **count** (occurrence count of words)\n",
    "\n",
    "- **freq** (frewquency of occurrence within a document)\n",
    "\n",
    "- **tfidf** (Term Frequency - Inverse Document Frequency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from pandas import DataFrame\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# load a movie review into memory\n",
    "def load_review(filename):\n",
    "    # open file in read only mode\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# extract tokens from review document\n",
    "def extract_tokens(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # regular expression for identification of punctuation characters\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation characters from each token\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # remove stop word tokens\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # remove short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "\n",
    "# load a review and clean it\n",
    "# return only those tokens that are also in the vocabulary\n",
    "def review_to_vocab(filename, vocab):\n",
    "    # load the review\n",
    "    review = load_review(filename)\n",
    "    # clean the review\n",
    "    tokens = extract_tokens(review)\n",
    "    # keep only those tokens that are also in the vocabulary\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# process all reviews in a directory\n",
    "def process_reviews(directory, vocab, is_train):\n",
    "    lines = list()\n",
    "    # traverse files in the directory\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load and clean the doc\n",
    "        line = review_to_vocab(path, vocab)\n",
    "        # add to list\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "\n",
    "# load and process a dataset\n",
    "def process_dataset(vocab, is_train):\n",
    "    # load documents\n",
    "    # positive reviews\n",
    "    positive = process_reviews('txt_sentoken/pos', vocab, is_train)\n",
    "    # positive reviews\n",
    "    negative = process_reviews('txt_sentoken/neg', vocab, is_train)\n",
    "    docs = positive + negative\n",
    "    # prepare class labels - 0 for nesgtaive review and 1 for positive review\n",
    "    labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))]\n",
    "    return docs, labels\n",
    "\n",
    "\n",
    "# define MLP model\n",
    "def define_model(n_words):\n",
    "    # define the model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# evaluate the MLP model\n",
    "def evaluate_model(Xtrain, ytrain, Xtest, ytest):\n",
    "    scores = list()\n",
    "    repeats = 10\n",
    "    words = Xtest.shape[1]\n",
    "    for i in range(repeats):\n",
    "    # define the model\n",
    "        model = define_model(words)\n",
    "        # fit the model\n",
    "        model.fit(Xtrain, ytrain, epochs=10, verbose=0)\n",
    "        # evaluate the model\n",
    "        _, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "        scores.append(acc)\n",
    "        print('%d accuracy: %s' % ((i+1), acc))\n",
    "    return scores\n",
    "\n",
    "# prepare BoW encoding of reviews\n",
    "def prepare_reviews(train_docs, test_docs, mode):\n",
    "    # create a tokenizer\n",
    "    tokenizer = Tokenizer()\n",
    "    # fit the tokenizer on the documents\n",
    "    tokenizer.fit_on_texts(train_docs)\n",
    "    # encode training dataset\n",
    "    Xtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\n",
    "    # encode test dataset\n",
    "    Xtest = tokenizer.texts_to_matrix(test_docs, mode=mode)\n",
    "    return Xtrain, Xtest\n",
    "\n",
    "# load vocabulary\n",
    "filename = 'vocabulary.txt'\n",
    "vocabulary = load_doc(filename)\n",
    "vocab = set(vocabulary.split())\n",
    "\n",
    "# load all reviews\n",
    "train_docs, ytrain = process_dataset(vocab, True)\n",
    "test_docs, ytest = process_dataset(vocab, False)\n",
    "\n",
    "# test with different word scoring schemes\n",
    "modes = ['binary', 'count', 'tfidf', 'freq']\n",
    "results = DataFrame()\n",
    "\n",
    "for mode in modes:\n",
    "    # prepare data for mode\n",
    "    Xtrain, Xtest = prepare_reviwes(train_docs, test_docs, mode)\n",
    "    # evaluate the model for a word scoring scheme\n",
    "    results[mode] = evaluate_model(Xtrain, ytrain, Xtest, ytest)\n",
    "\n",
    "# summarize results\n",
    "print(results.describe())\n",
    "\n",
    "# plot results\n",
    "results.boxplot()\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Word Embedding Model\n",
    "\n",
    "- Words with similar meaning to have a similar representation.\n",
    "\n",
    "- These are **distributed representations**.\n",
    "\n",
    "- The number of **features** is much smaller than the size of the vocabulary. Therefore, **dense representation**.\n",
    "\n",
    "- Individual elements in a vector are not mutually exclusive. \n",
    "\n",
    "- Elements come together to represent concepts. \n",
    "\n",
    "- Each configuration of the vector represents a different concept.\n",
    "\n",
    "- Each word is represented by a point in the embedding space.\n",
    "\n",
    "- Words with similar meanings are locally clustered within the space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding Algorithms\n",
    "\n",
    "- Embedding Layer\n",
    "\n",
    "- Word2Vec (CBoW and SkipGram)\n",
    "\n",
    "- Global Vectors for Word Representation (GloVe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning a Word Embedding\n",
    "\n",
    "- Emedding is learned in isolation and used in several models\n",
    "\n",
    "- Emedding is learned for a specific task/model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing an Embedding\n",
    "\n",
    "- Static (a pre-trained does not change over the life of a model).\n",
    "\n",
    "- Updated (a pre-trained embedding is used to seed the model, but the embedding is updated during the training of the model).\n",
    "\n",
    "- **Word2Vec** and **GloVe** embeddings are available for free download."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GenSim Python Library\n",
    "\n",
    "- Gensim is a suite of NLP tools for topic modeling.\n",
    "\n",
    "- Gensim supports an implementation of the Word2Vec word embedding.\n",
    "\n",
    "- **sudo pip install -U gensim**\n",
    "\n",
    "- Two training algorithms for learning embeddings from text:\n",
    "\n",
    "    - Continuous Bag-of-Words (CBOW) \n",
    "    \n",
    "    - Skipgrams\n",
    "    \n",
    "- These algorithms consider a window of words for each target word to provide context.\n",
    "\n",
    "- GenSim provides the **Word2Vec** class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example to lillustrate learning word embeddings using GenSim\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# training data\n",
    "sentences = [['from', 'a', 'broader', 'perspective', 'there', 'are', 'three', 'classes', 'of', 'languages', 'spoken', 'written', 'and', 'sign'],\n",
    "            ['it', 'is', 'believed', 'that', 'spoken', 'languages', 'preceded', 'the', 'development', 'of', 'written', 'languages', 'and', 'sign', 'languages', 'came', 'much', 'later'],\n",
    "            ['language', 'is', 'social', 'as', 'much', 'as', 'individual'],\n",
    "            ['the', 'social', 'aspect', 'is', 'for', 'communication', 'whereas', 'the', 'individual', 'aspect', 'is', 'a', 'medium', 'for', 'thought'],\n",
    "            ['also', 'whether', 'thought', 'is', 'independent', 'of', 'language', 'is', 'a', 'philosophical', 'question'],\n",
    "            ['some', 'believe', 'that', 'language', 'and', 'culture', 'are', 'intertwined']]\n",
    "\n",
    "# train the embedding model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "\n",
    "# summarize the model\n",
    "print(model)\n",
    "\n",
    "# summarize model vocabulary (model is accessed via wv)\n",
    "words = list(model.wv.vocab)\n",
    "print(words)\n",
    "\n",
    "# access vector for one word\n",
    "print(model['language'])\n",
    "\n",
    "# save model to disk\n",
    "model.save('Word2Vec.bin')\n",
    "\n",
    "# load model\n",
    "new_model = Word2Vec.load('Word2Vec.bin')\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize word embeddings\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# training data\n",
    "sentences = [['from', 'a', 'broader', 'perspective', 'there', 'are', 'three', 'classes', 'of', 'languages', 'spoken', 'written', 'and', 'sign'],\n",
    "            ['it', 'is', 'believed', 'that', 'spoken', 'languages', 'preceded', 'the', 'development', 'of', 'written', 'languages', 'and', 'sign', 'languages', 'came', 'much', 'later'],\n",
    "            ['language', 'is', 'social', 'as', 'much', 'as', 'individual'],\n",
    "            ['the', 'social', 'aspect', 'is', 'for', 'communication', 'whereas', 'the', 'individual', 'aspect', 'is', 'a', 'medium', 'for', 'thought'],\n",
    "            ['also', 'whether', 'thought', 'is', 'independent', 'of', 'language', 'is', 'a', 'philosophical', 'question'],\n",
    "            ['some', 'believe', 'that', 'language', 'and', 'culture', 'are', 'intertwined']]\n",
    "\n",
    "# train the embedding model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "\n",
    "# fit a 2nd PCA model to the vectors\n",
    "X = model[model.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "\n",
    "# create a scatter plot of the projection\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "words = list(model.wv.vocab)\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "    \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google's Word2Vec Embedding\n",
    "\n",
    "- The pre-trained Google Word2Vec model was trained on Google news data (about 100\n",
    "billion words)\n",
    "\n",
    "- It contains 3 million words and phrases and was fit using 300-dimensional word\n",
    "vectors. \n",
    "\n",
    "- It is a 1.53 Gb file, in unzipped form 3.4 Gb.\n",
    "\n",
    "- Download it from [here](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arithmetic with Google Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# load the google word2vec model\n",
    "filename = 'GoogleNews-vectors-negative300.bin'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=True)\n",
    "\n",
    "# calculate: (king - man) + woman = ?\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
